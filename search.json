[{"path":"https://merck.github.io/wpgsd/articles/adj-seq-p.html","id":"example-overview","dir":"Articles","previous_headings":"","what":"Example overview","title":"Adjusted sequential p-values","text":"2-arm controlled clinical trial example one primary endpoint, 3 patient populations defined status two biomarkers B: biomarker positive, biomarker B positive, overall population. 3 primary elementary hypotheses : H1H_1: experimental treatment superior control biomarker positive population; H2H_2: experimental treatment superior control biomarker B positive population; H3H_3: experimental treatment superior control overall population. Assume interim analysis final analysis planned study number events listed observed p-values information fraction H1,H2,H3H_1, H_2, H_3 IA assign initial weights H1,H2,H3H_1, H_2, H_3 (w1(),w2(),w3())=(0.3,0.3,0.4).\\left(w_1(), w_2(), w_3() \\right) = (0.3, 0.3, 0.4). multiplicity strategy visualized . H1H_1 rejected, 3/73/7 local significance level α1\\alpha_1 propagated H2H_2, 4/74/7 go H3H_3. H3H_3 rejected, half α3\\alpha_3 goes H1H_1, half goes H2H_2.  correlation 6 statistisc (2 analysis ×\\times 3 hypothesis) ","code":"event_tbl <- tribble(   ~population, ~analysis, ~event,   \"A positive\", 1, 100,   \"B positive\", 1, 110,   \"AB positive\", 1, 80,   \"overall\", 1, 225,   \"A positive\", 2, 200,   \"B positive\", 2, 220,   \"AB positive\", 2, 160,   \"overall\", 2, 450, ) obs_tbl <- tribble(   ~hypothesis, ~analysis, ~obs_p,   \"H1\", 1, 0.02,   \"H2\", 1, 0.01,   \"H3\", 1, 0.012,   \"H1\", 2, 0.015,   \"H2\", 2, 0.012,   \"H3\", 2, 0.010 ) %>%   mutate(obs_Z = -qnorm(obs_p))  obs_tbl %>%   gt() %>%   tab_header(title = \"Nominal p-values\") p_obs_IA <- (obs_tbl %>% filter(analysis == 1))$obs_p p_obs_FA <- (obs_tbl %>% filter(analysis == 2))$obs_p IF_IA <- c(   ((event_tbl %>% filter(analysis == 1, population == \"A positive\"))$event + (event_tbl %>% filter(analysis == 1, population == \"overall\"))$event) /     ((event_tbl %>% filter(analysis == 2, population == \"A positive\"))$event + (event_tbl %>% filter(analysis == 2, population == \"overall\"))$event),   ((event_tbl %>% filter(analysis == 1, population == \"B positive\"))$event + (event_tbl %>% filter(analysis == 1, population == \"overall\"))$event) /     ((event_tbl %>% filter(analysis == 2, population == \"B positive\"))$event + (event_tbl %>% filter(analysis == 2, population == \"overall\"))$event),   ((event_tbl %>% filter(analysis == 1, population == \"AB positive\"))$event + (event_tbl %>% filter(analysis == 1, population == \"overall\"))$event) /     ((event_tbl %>% filter(analysis == 2, population == \"AB positive\"))$event + (event_tbl %>% filter(analysis == 2, population == \"overall\"))$event) )  IF_IA ## [1] 0.5 0.5 0.5 # Transition matrix in Figure A1 m <- matrix(c(   0, 3 / 7, 4 / 7,   3 / 7, 0, 4 / 7,   0.5, 0.5, 0 ), nrow = 3, byrow = TRUE) # Initial weights w <- c(0.3, 0.3, 0.4) name_hypotheses <- c(\"H1: Biomarker A positive\", \"H2: Biomarker B positive\", \"H3: Overall Population\")  hplot <- gMCPLite::hGraph(   3,   alphaHypotheses = w, m = m,   nameHypotheses = name_hypotheses, trhw = .2, trhh = .1,   digits = 5, trdigits = 3, size = 5, halfWid = 1, halfHgt = 0.5,   offset = 0.2, trprop = 0.4,   fill = as.factor(c(2, 3, 1)),   palette = c(\"#BDBDBD\", \"#E0E0E0\", \"#EEEEEE\"),   wchar = \"w\" ) hplot # Event count of intersection of paired hypotheses - Table 2 # H1, H2: Hypotheses intersected. # (1, 1) represents counts for hypothesis 1 # (1, 2) for counts for the intersection of hypotheses 1 and 2 event <- tribble(   ~H1, ~H2, ~Analysis, ~Event,   1, 1, 1, event_tbl %>% filter(analysis == 1, population == \"A positive\") %>% select(event) %>% as.numeric(),   2, 2, 1, event_tbl %>% filter(analysis == 1, population == \"B positive\") %>% select(event) %>% as.numeric(),   3, 3, 1, event_tbl %>% filter(analysis == 1, population == \"overall\") %>% select(event) %>% as.numeric(),   1, 2, 1, event_tbl %>% filter(analysis == 1, population == \"AB positive\") %>% select(event) %>% as.numeric(),   1, 3, 1, event_tbl %>% filter(analysis == 1, population == \"A positive\") %>% select(event) %>% as.numeric(),   2, 3, 1, event_tbl %>% filter(analysis == 1, population == \"B positive\") %>% select(event) %>% as.numeric(),   1, 1, 2, event_tbl %>% filter(analysis == 2, population == \"A positive\") %>% select(event) %>% as.numeric(),   2, 2, 2, event_tbl %>% filter(analysis == 2, population == \"B positive\") %>% select(event) %>% as.numeric(),   3, 3, 2, event_tbl %>% filter(analysis == 2, population == \"overall\") %>% select(event) %>% as.numeric(),   1, 2, 2, event_tbl %>% filter(analysis == 2, population == \"AB positive\") %>% select(event) %>% as.numeric(),   1, 3, 2, event_tbl %>% filter(analysis == 2, population == \"A positive\") %>% select(event) %>% as.numeric(),   2, 3, 2, event_tbl %>% filter(analysis == 2, population == \"B positive\") %>% select(event) %>% as.numeric() ) event ## # A tibble: 12 × 4 ##       H1    H2 Analysis Event ##    <dbl> <dbl>    <dbl> <dbl> ##  1     1     1        1   100 ##  2     2     2        1   110 ##  3     3     3        1   225 ##  4     1     2        1    80 ##  5     1     3        1   100 ##  6     2     3        1   110 ##  7     1     1        2   200 ##  8     2     2        2   220 ##  9     3     3        2   450 ## 10     1     2        2   160 ## 11     1     3        2   200 ## 12     2     3        2   220 # Generate correlation from events gs_corr <- wpgsd::generate_corr(event) gs_corr %>% round(2) ##      H1_A1 H2_A1 H3_A1 H1_A2 H2_A2 H3_A2 ## [1,]  1.00  0.76  0.67  0.71  0.54  0.47 ## [2,]  0.76  1.00  0.70  0.54  0.71  0.49 ## [3,]  0.67  0.70  1.00  0.47  0.49  0.71 ## [4,]  0.71  0.54  0.47  1.00  0.76  0.67 ## [5,]  0.54  0.71  0.49  0.76  1.00  0.70 ## [6,]  0.47  0.49  0.71  0.67  0.70  1.00"},{"path":[]},{"path":"https://merck.github.io/wpgsd/articles/adj-seq-p.html","id":"ia","dir":"Articles","previous_headings":"Sequential p-value","what":"IA","title":"Adjusted sequential p-values","text":"","code":"seq_p_IA_H123 <- calc_seq_p(   test_analysis = 1,   test_hypothesis = \"H1, H2, H3\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ),   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.2) )  seq_p_IA_H12 <- calc_seq_p(   test_analysis = 1,   test_hypothesis = \"H1, H2\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ),   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.2) )  seq_p_IA_H13 <- calc_seq_p(   test_analysis = 1,   test_hypothesis = \"H1, H3\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ),   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.2) )  seq_p_IA_H23 <- calc_seq_p(   test_analysis = 1, # stage of interest   test_hypothesis = \"H2, H3\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ),   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.2) )  seq_p_IA_H1 <- calc_seq_p(   test_analysis = 1,   test_hypothesis = \"H1\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ),   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.2) )  seq_p_IA_H2 <- calc_seq_p(   test_analysis = 1,   test_hypothesis = \"H2\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ),   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.2) )  seq_p_IA_H3 <- calc_seq_p(   test_analysis = 1,   test_hypothesis = \"H3\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ),   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.2) ) seq_p_IA_H123_B <- calc_seq_p(   test_analysis = 1, # stage of interest   test_hypothesis = \"H1, H2, H3\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.3) )  seq_p_IA_H12_B <- calc_seq_p(   test_analysis = 1, # stage of interest   test_hypothesis = \"H1, H2\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2) )  seq_p_IA_H13_B <- calc_seq_p(   test_analysis = 1, # stage of interest   test_hypothesis = \"H1, H3\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.3) )  seq_p_IA_H23_B <- calc_seq_p(   test_analysis = 1, # stage of interest   test_hypothesis = \"H2, H3\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.3) )  seq_p_IA_H1_B <- calc_seq_p(   test_analysis = 1, # stage of interest   test_hypothesis = \"H1\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.3) )  seq_p_IA_H2_B <- calc_seq_p(   test_analysis = 1, # stage of interest   test_hypothesis = \"H2\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.3) )  seq_p_IA_H3_B <- calc_seq_p(   test_analysis = 1, # stage of interest   test_hypothesis = \"H3\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.3) )"},{"path":"https://merck.github.io/wpgsd/articles/adj-seq-p.html","id":"fa","dir":"Articles","previous_headings":"Sequential p-value","what":"FA","title":"Adjusted sequential p-values","text":"","code":"seq_p_FA_H123 <- calc_seq_p(   test_analysis = 2, # stage of interest   test_hypothesis = \"H1, H2, H3\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.15) )  seq_p_FA_H12 <- calc_seq_p(   test_analysis = 2, # stage of interest   test_hypothesis = \"H1, H2\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.15) )  seq_p_FA_H13 <- calc_seq_p(   test_analysis = 2, # stage of interest   test_hypothesis = \"H1, H3\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.15) )  seq_p_FA_H23 <- calc_seq_p(   test_analysis = 2, # stage of interest   test_hypothesis = \"H2, H3\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.15) )  seq_p_FA_H1 <- calc_seq_p(   test_analysis = 2, # stage of interest   test_hypothesis = \"H1\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.2) )  seq_p_FA_H2 <- calc_seq_p(   test_analysis = 2, # stage of interest   test_hypothesis = \"H2\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.2) )  seq_p_FA_H3 <- calc_seq_p(   test_analysis = 2, # stage of interest   test_hypothesis = \"H3\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.2) ) seq_p_FA_H123_B <- calc_seq_p(   test_analysis = 2, # stage of interest   test_hypothesis = \"H1, H2, H3\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2) )  seq_p_FA_H12_B <- calc_seq_p(   test_analysis = 2, # stage of interest   test_hypothesis = \"H1, H2\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2) )  seq_p_FA_H13_B <- calc_seq_p(   test_analysis = 2, # stage of interest   test_hypothesis = \"H1, H3\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2) )  seq_p_FA_H23_B <- calc_seq_p(   test_analysis = 2, # stage of interest   test_hypothesis = \"H2, H3\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2) )  seq_p_FA_H1_B <- calc_seq_p(   test_analysis = 2, # stage of interest   test_hypothesis = \"H1\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2) )  seq_p_FA_H2_B <- calc_seq_p(   test_analysis = 2, # stage of interest   test_hypothesis = \"H2\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2) )  seq_p_FA_H3_B <- calc_seq_p(   test_analysis = 2, # stage of interest   test_hypothesis = \"H3\",   p_obs = tibble(     analysis = 1:2,     H1 = c(p_obs_IA[1], p_obs_FA[1]),     H2 = c(p_obs_IA[2], p_obs_FA[2]),     H3 = c(p_obs_IA[3], p_obs_FA[3])   ), # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2) )"},{"path":[]},{"path":"https://merck.github.io/wpgsd/articles/adj-seq-p.html","id":"ia-1","dir":"Articles","previous_headings":"Adjusted-Sequential p-value","what":"IA","title":"Adjusted sequential p-values","text":"","code":"adj_seq_IA_H1 <- max(seq_p_IA_H123, seq_p_IA_H12, seq_p_IA_H13, seq_p_IA_H1) adj_seq_IA_H2 <- max(seq_p_IA_H123, seq_p_IA_H12, seq_p_IA_H23, seq_p_IA_H2) adj_seq_IA_H3 <- max(seq_p_IA_H123, seq_p_IA_H13, seq_p_IA_H23, seq_p_IA_H3)  cat(\"The adjusted-sequential p-value of H1, H2, H3 in IA via WPGSD is\", adj_seq_IA_H1, adj_seq_IA_H2, adj_seq_IA_H3, \"\\n\") ## The adjusted-sequential p-value of H1, H2, H3 in IA via WPGSD is 0.1942576 0.1942576 0.1942576 adj_seq_IA_H1_B <- max(seq_p_IA_H123_B, seq_p_IA_H12_B, seq_p_IA_H13_B, seq_p_IA_H1_B) adj_seq_IA_H2_B <- max(seq_p_IA_H123_B, seq_p_IA_H12_B, seq_p_IA_H23_B, seq_p_IA_H2_B) adj_seq_IA_H3_B <- max(seq_p_IA_H123_B, seq_p_IA_H13_B, seq_p_IA_H23_B, seq_p_IA_H3_B)  cat(\"The adjusted-sequential p-value of H1, H2, H3 in FA via weighted Bonferroni is\", adj_seq_IA_H1_B, adj_seq_IA_H2_B, adj_seq_IA_H3_B, \"\\n\") ## The adjusted-sequential p-value of H1, H2, H3 in FA via weighted Bonferroni is 0.2516717 0.2516717 0.2516717"},{"path":[]},{"path":"https://merck.github.io/wpgsd/articles/adj-seq-p.html","id":"wpgsd","dir":"Articles","previous_headings":"Adjusted-Sequential p-value > FA","what":"WPGSD","title":"Adjusted sequential p-values","text":"","code":"adj_seq_FA_H1 <- max(seq_p_FA_H123, seq_p_FA_H12, seq_p_FA_H13, seq_p_FA_H1) adj_seq_FA_H2 <- max(seq_p_FA_H123, seq_p_FA_H12, seq_p_FA_H23, seq_p_FA_H2) adj_seq_FA_H3 <- max(seq_p_FA_H123, seq_p_FA_H13, seq_p_FA_H23, seq_p_FA_H3)  cat(\"The adjusted-sequential p-value of H1, H2, H3 in FA via WPGSD is\", adj_seq_FA_H1, adj_seq_FA_H2, adj_seq_FA_H3, \"\\n\") ## The adjusted-sequential p-value of H1, H2, H3 in FA via WPGSD is 0.02096985 0.02096985 0.02067307 adj_seq_FA_H1_B <- max(seq_p_FA_H123_B, seq_p_FA_H12_B, seq_p_FA_H13_B, seq_p_FA_H1_B) adj_seq_FA_H2_B <- max(seq_p_FA_H123_B, seq_p_FA_H12_B, seq_p_FA_H23_B, seq_p_FA_H2_B) adj_seq_FA_H3_B <- max(seq_p_FA_H123_B, seq_p_FA_H13_B, seq_p_FA_H23_B, seq_p_FA_H3_B)  cat(\"The adjusted-sequential p-value of H1, H2, H3 in FA via weighted Bonferroni is\", adj_seq_FA_H1_B, adj_seq_FA_H2_B, adj_seq_FA_H3_B, \"\\n\") ## The adjusted-sequential p-value of H1, H2, H3 in FA via weighted Bonferroni is 0.0265823 0.0265823 0.0265823"},{"path":"https://merck.github.io/wpgsd/articles/adj-seq-p.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Adjusted sequential p-values","text":"","code":"ans <- tribble(   ~Analysis, ~Hypothesis, ~`Sequential p-values of WPGSD`, ~`Sequential p-values of Weighted Bonferroni`, ~`Adjusted-sequential p-values of WPGSD`, ~`Adjusted-sequential p-values of Weighted Bonferroni`,   \"IA\", \"H123\", seq_p_IA_H123, seq_p_IA_H123_B, NA, NA,   \"IA\", \"H12\", seq_p_IA_H12, seq_p_IA_H12_B, NA, NA,   \"IA\", \"H13\", seq_p_IA_H13, seq_p_IA_H13_B, NA, NA,   \"IA\", \"H23\", seq_p_IA_H23, seq_p_IA_H23_B, NA, NA,   \"IA\", \"H1\", seq_p_IA_H1, seq_p_IA_H1_B, adj_seq_IA_H1, adj_seq_IA_H1_B,   \"IA\", \"H2\", seq_p_IA_H2, seq_p_IA_H2_B, adj_seq_IA_H2, adj_seq_IA_H2_B,   \"IA\", \"H3\", seq_p_IA_H3, seq_p_IA_H3_B, adj_seq_IA_H3, adj_seq_IA_H3_B,   \"FA\", \"H123\", seq_p_FA_H123, seq_p_FA_H123_B, NA, NA,   \"FA\", \"H12\", seq_p_FA_H12, seq_p_FA_H12_B, NA, NA,   \"FA\", \"H13\", seq_p_FA_H13, seq_p_FA_H13_B, NA, NA,   \"FA\", \"H23\", seq_p_FA_H23, seq_p_FA_H23_B, NA, NA,   \"FA\", \"H1\", seq_p_FA_H1, seq_p_FA_H1_B, adj_seq_FA_H1, adj_seq_FA_H1_B,   \"FA\", \"H2\", seq_p_FA_H2, seq_p_FA_H2_B, adj_seq_FA_H2, adj_seq_FA_H2_B,   \"FA\", \"H3\", seq_p_FA_H3, seq_p_FA_H3_B, adj_seq_FA_H3, adj_seq_FA_H3_B )  ans %>%   select(     Analysis, Hypothesis,     `Sequential p-values of Weighted Bonferroni`, `Adjusted-sequential p-values of Weighted Bonferroni`,     `Sequential p-values of WPGSD`, `Adjusted-sequential p-values of WPGSD`   ) %>%   gt() %>%   tab_spanner(     label = \"Weighted Bonferroni\",     columns = c(`Sequential p-values of Weighted Bonferroni`, `Adjusted-sequential p-values of Weighted Bonferroni`)   ) %>%   tab_spanner(     label = \"WPGSD\",     columns = c(`Sequential p-values of WPGSD`, `Adjusted-sequential p-values of WPGSD`)   ) %>%   tab_style_body(     columns = where(is.numeric),     style = cell_fill(color = \"pink\"),     fn = function(x) x <= 0.025   ) %>%   fmt_number(columns = 3:6, decimals = 4) %>%   tab_header(     title = \"(Adjusted-) sequential p-values\",     subtitle = \"Multiple populations\"   ) # %>% as_latex()"},{"path":"https://merck.github.io/wpgsd/articles/corr_calculation.html","id":"methodologies-to-calculate-correlations","dir":"Articles","previous_headings":"","what":"Methodologies to calculate correlations","title":"Correlated test statistics","text":"Suppose group sequential trial mm elementary null hypotheses HiH_i, ∈=1,...,mi \\={1,...,m}, KK analyses. Let kk index interim analyses final analyses, k=1,2,...Kk=1,2,...K. nonempty set J⊆IJ \\subseteq , denote intersection hypothesis HJ=∩j∈JHjH_J=\\cap_{j \\J}H_j. note HIH_I global null hypothesis. assume plan hypotheses tested kk planned analyses trial continues end hypotheses. assume distribution m×Km \\times K tests mm individual hypotheses kk analyses multivariate normal completely known correlation matrix. Let ZikZ_{ik} standardized normal test statistic hypothesis ∈Ii \\, analysis 1≤k≤K1 \\le k \\le K. Let nikn_{ik} number events collected cumulatively stage kk hypothesis ii. ni∧′,k∧k′n_{\\wedge ',k \\wedge k'} number events included ZikZ_{ik} ii, ′∈Ii' \\, 1≤k1 \\le k, k′≤Kk' \\le K. key parametric tests utilize correlation among test statistics. correlation ZikZ_{ik} Zi′k′Z_{'k'} Corr(Zik,Zi′k′)=ni∧′,k∧k′nik*ni′k′Corr(Z_{ik},Z_{'k'})=\\frac{n_{\\wedge ',k \\wedge k'}}{\\sqrt{n_{ik}*n_{'k'}}}.","code":""},{"path":"https://merck.github.io/wpgsd/articles/corr_calculation.html","id":"examples","dir":"Articles","previous_headings":"","what":"Examples","title":"Correlated test statistics","text":"borrow example paper Anderson et al. (Anderson et al. (2022)), demonstrated Section 2 - Motivating Examples, use Example 1 basis . setting : two-arm controlled clinical trial one primary endpoint, three patient populations defined status two biomarkers, B: Biomarker positive, population 1, Biomarker B positive, population 2, Overall population. 3 primary elementary hypotheses : H1: experimental treatment superior control population 1 H2: experimental treatment superior control population 2 H3: experimental treatment superior control overall population Assume interim analysis final analysis planned study. number events listed ","code":"library(dplyr) library(tibble) library(gt) event_tb <- tribble(   ~Population, ~\"Number of Event in IA\", ~\"Number of Event in FA\",   \"Population 1\", 100, 200,   \"Population 2\", 110, 220,   \"Overlap of Population 1 and 2\", 80, 160,   \"Overall Population\", 225, 450 ) event_tb %>%   gt() %>%   tab_header(title = \"Number of events at each population\")"},{"path":"https://merck.github.io/wpgsd/articles/corr_calculation.html","id":"correlation-of-different-populations-within-the-same-analysis","dir":"Articles","previous_headings":"Examples","what":"Correlation of different populations within the same analysis","title":"Correlated test statistics","text":"Let’s consider simple situation, want compare population 1 population 2 interim analyses. k=1k=1, compare H1H_{1} H2H_{2}, ii =1i=1 =2i=2. correlation matrix Corr(Z11,Z21)=n1∧2,1∧1n11*n21Corr(Z_{11},Z_{21})=\\frac{n_{1 \\wedge 2,1 \\wedge 1}}{\\sqrt{n_{11}*n_{21}}} number events listed corrleation simply calculated Corr(Z11,Z21)=80100*110=0.76Corr(Z_{11},Z_{21})=\\frac{80}{\\sqrt{100*110}}=0.76","code":"event_tbl <- tribble(   ~Population, ~\"Number of Event in IA\",   \"Population 1\", 100,   \"Population 2\", 110,   \"Overlap in population 1 and 2\", 80 ) event_tbl %>%   gt() %>%   tab_header(title = \"Number of events at each population in example 1\") Corr1 <- 80 / sqrt(100 * 110) round(Corr1, 2) ## [1] 0.76"},{"path":"https://merck.github.io/wpgsd/articles/corr_calculation.html","id":"correlation-of-different-analyses-within-the-same-population","dir":"Articles","previous_headings":"Examples","what":"Correlation of different analyses within the same population","title":"Correlated test statistics","text":"Let’s consider another simple situation, want compare single population, example, population 1, different analyses, interim final analyses. =1i=1, compare IA FA, kk k=1k=1 k=2k=2. correlation matrix Corr(Z11,Z12)=n1∧1,1∧2n11*n12Corr(Z_{11},Z_{12})=\\frac{n_{1 \\wedge 1,1 \\wedge 2}}{\\sqrt{n_{11}*n_{12}}} number events listed corrleation simply calculated Corr(Z11,Z12)=100100*200=0.71\\text{Corr}(Z_{11},Z_{12})=\\frac{100}{\\sqrt{100*200}}=0.71 100 numerator overlap number events interim analysis final analysis population 1.","code":"event_tb2 <- tribble(   ~Population, ~\"Number of Event in IA\", ~\"Number of Event in FA\",   \"Population 1\", 100, 200 ) event_tb2 %>%   gt() %>%   tab_header(title = \"Number of events at each analyses in example 2\") Corr1 <- 100 / sqrt(100 * 200) round(Corr1, 2) ## [1] 0.71"},{"path":"https://merck.github.io/wpgsd/articles/corr_calculation.html","id":"correlation-of-different-analyses-and-different-population","dir":"Articles","previous_headings":"Examples","what":"Correlation of different analyses and different population","title":"Correlated test statistics","text":"Let’s consider situation want compare population 1 interim analyses population 2 final analyses. different population, =1i=1 =2i=2, compare IA FA, kk k=1k=1 k=2k=2. correlation matrix Corr(Z11,Z22)=n1∧1,2∧2n11*n22\\text{Corr}(Z_{11},Z_{22})=\\frac{n_{1 \\wedge 1,2 \\wedge 2}}{\\sqrt{n_{11}*n_{22}}} number events listed correlation simply calculated Corr(Z11,Z22)=80100*220=0.54\\text{Corr}(Z_{11},Z_{22})=\\frac{80}{\\sqrt{100*220}}=0.54 80 numerator overlap number events population 1 interim analysis population 2 final analysis.","code":"event_tb3 <- tribble(   ~Population, ~\"Number of Event in IA\", ~\"Number of Event in FA\",   \"Population 1\", 100, 200,   \"Population 2\", 110, 220,   \"Overlap in population 1 and 2\", 80, 160 ) event_tb3 %>%   gt() %>%   tab_header(title = \"Number of events at each population & analyses in example 3\") Corr1 <- 80 / sqrt(100 * 220) round(Corr1, 2) ## [1] 0.54"},{"path":"https://merck.github.io/wpgsd/articles/corr_calculation.html","id":"generate-the-correlation-matrix-by-generate_corr","dir":"Articles","previous_headings":"","what":"Generate the correlation matrix by generate_corr()","title":"Correlated test statistics","text":"Now know calculate correlation values different situations, generate_corr() function built based logic. can directly calculate results cross situation via function. First, need event table including information study. H1 refers one hypothesis, selected depending interest, H2 refers hypothesis, listed multiplicity testing. example, H1 means experimental treatment superior control population 1/experimental arm 1; H2 means experimental treatment superior control population 2/experimental arm 2; Analysis means different analysis stages, example, 1 means interim analysis, 2 means final analysis; Event common events overlap H1 H2. example: H1=1, H2=1, Analysis=1, Event=100indicates first population, 100 cases experimental treatment superior control interim analysis. Another example: H1=1, H2=2, Analysis=2, Event=160 indicates number overlapping cases experimental treatment superior control population 1 2 final analysis 160. noticed, column names function fixed H1, H2, Analysis, Event. input event table function generate_corr(), get correlation matrix follow.","code":"library(wpgsd) # The event table event <- tibble::tribble(   ~H1, ~H2, ~Analysis, ~Event,   1, 1, 1, 100,   2, 2, 1, 110,   3, 3, 1, 225,   1, 2, 1, 80,   1, 3, 1, 100,   2, 3, 1, 110,   1, 1, 2, 200,   2, 2, 2, 220,   3, 3, 2, 450,   1, 2, 2, 160,   1, 3, 2, 200,   2, 3, 2, 220 )  event %>%   gt() %>%   tab_header(title = \"Number of events at each population & analyses\") generate_corr(event) ##          H1_A1     H2_A1     H3_A1     H1_A2     H2_A2     H3_A2 ## [1,] 1.0000000 0.7627701 0.6666667 0.7071068 0.5393599 0.4714045 ## [2,] 0.7627701 1.0000000 0.6992059 0.5393599 0.7071068 0.4944132 ## [3,] 0.6666667 0.6992059 1.0000000 0.4714045 0.4944132 0.7071068 ## [4,] 0.7071068 0.5393599 0.4714045 1.0000000 0.7627701 0.6666667 ## [5,] 0.5393599 0.7071068 0.4944132 0.7627701 1.0000000 0.6992059 ## [6,] 0.4714045 0.4944132 0.7071068 0.6666667 0.6992059 1.0000000"},{"path":[]},{"path":"https://merck.github.io/wpgsd/articles/tech-detail-wb.html","id":"example-overview","dir":"Articles","previous_headings":"","what":"Example overview","title":"Procedure to compute p-value boundaries by weighted Bonferroni","text":"2-arm controlled clinical trial example one primary endpoint, 3 patient populations defined status two biomarkers B: biomarker positive, biomarker B positive, overall population. 3 primary elementary hypotheses : H1H_1: experimental treatment superior control biomarker positive population; H2H_2: experimental treatment superior control biomarker B positive population; H3H_3: experimental treatment superior control overall population. Assume interim analysis final analysis planned study number events listed ","code":"k <- 2 # Number of total analysis n_hypotheses <- 3 # Number of hypotheses"},{"path":"https://merck.github.io/wpgsd/articles/tech-detail-wb.html","id":"observed-p-values","dir":"Articles","previous_headings":"Example overview","what":"Observed p-values","title":"Procedure to compute p-value boundaries by weighted Bonferroni","text":"","code":"obs_tbl <- tribble(   ~hypothesis, ~analysis, ~obs_p,   \"H1\", 1, 0.02,   \"H2\", 1, 0.01,   \"H3\", 1, 0.006,   \"H1\", 2, 0.015,   \"H2\", 2, 0.012,   \"H3\", 2, 0.004 ) %>%   mutate(obs_Z = -qnorm(obs_p))  obs_tbl %>%   gt() %>%   tab_header(title = \"Nominal p-values\") p_obs_IA <- (obs_tbl %>% filter(analysis == 1))$obs_p p_obs_FA <- (obs_tbl %>% filter(analysis == 2))$obs_p"},{"path":"https://merck.github.io/wpgsd/articles/tech-detail-wb.html","id":"information-fraction","dir":"Articles","previous_headings":"Example overview","what":"Information fraction","title":"Procedure to compute p-value boundaries by weighted Bonferroni","text":"information fraction H1H_1, H2H_2, H3H_3 IA ","code":"alpha <- 0.025 event_tbl <- tribble(   ~population, ~analysis, ~event,   \"A positive\", 1, 80,   \"B positive\", 1, 88,   \"AB positive\", 1, 64,   \"overall\", 1, 180,   \"A positive\", 2, 160,   \"B positive\", 2, 176,   \"AB positive\", 2, 128,   \"overall\", 2, 360, ) IF_IA <- c(   ((event_tbl %>% filter(analysis == 1, population == \"A positive\"))$event + (event_tbl %>% filter(analysis == 1, population == \"overall\"))$event) /     ((event_tbl %>% filter(analysis == 2, population == \"A positive\"))$event + (event_tbl %>% filter(analysis == 2, population == \"overall\"))$event),   ((event_tbl %>% filter(analysis == 1, population == \"B positive\"))$event + (event_tbl %>% filter(analysis == 1, population == \"overall\"))$event) /     ((event_tbl %>% filter(analysis == 2, population == \"B positive\"))$event + (event_tbl %>% filter(analysis == 2, population == \"overall\"))$event),   ((event_tbl %>% filter(analysis == 1, population == \"AB positive\"))$event + (event_tbl %>% filter(analysis == 1, population == \"overall\"))$event) /     ((event_tbl %>% filter(analysis == 2, population == \"AB positive\"))$event + (event_tbl %>% filter(analysis == 2, population == \"overall\"))$event) )  IF_IA ## [1] 0.5 0.5 0.5"},{"path":"https://merck.github.io/wpgsd/articles/tech-detail-wb.html","id":"initial-weight-and-transition-matrix","dir":"Articles","previous_headings":"Example overview","what":"Initial weight and transition matrix","title":"Procedure to compute p-value boundaries by weighted Bonferroni","text":"assign initial weights H1H_1, H2H_2, H3H_3 (w1(),w2(),w3())=(0.3,0.3,0.4).\\left(w_1(), w_2(), w_3() \\right) = (0.3, 0.3, 0.4). multiplicity strategy visualized . H1H_1 rejected, 3/73/7 local significance level α1\\alpha_1 propagated H2H_2, 4/74/7 go H3H_3. H3H_3 rejected, half α3\\alpha_3 goes H1H_1, half goes H2H_2.","code":"m <- matrix(c( # Transition matrix   0, 3 / 7, 4 / 7,   3 / 7, 0, 4 / 7,   1 / 2, 1 / 2, 0 ), nrow = 3, byrow = TRUE)  w <- c(0.3, 0.3, 0.4) # Initial weights name_hypotheses <- c(   \"H1: Biomarker A positive\",   \"H2: Biomarker B positive\",   \"H3: Overall Population\" )  hplot <- gMCPLite::hGraph(   3,   alphaHypotheses = w, m = m,   nameHypotheses = name_hypotheses, trhw = .2, trhh = .1,   digits = 5, trdigits = 3, size = 5, halfWid = 1, halfHgt = 0.5,   offset = 0.2, trprop = 0.4,   fill = as.factor(c(2, 3, 1)),   palette = c(\"#BDBDBD\", \"#E0E0E0\", \"#EEEEEE\"),   wchar = \"w\" ) hplot # Get weights for all intersection hypotheses graph <- gMCPLite::matrix2graph(m) graph <- gMCPLite::setWeights(graph, w) # Set up hypothetical p-values (0 or 1) to obtain all combinations pvals <- NULL for (i in 1:n_hypotheses) {   if (i == 1) {     pvals <- data.frame(x = c(0, 1))     names(pvals) <- paste(\"pval_H\", i, sep = \"\")   } else {     tmp <- data.frame(x = c(0, 1))     names(tmp) <- paste(\"pval_H\", i, sep = \"\")     pvals <- merge(pvals, tmp)   } } # Get the weights for each intersection hypothesis inter_weight <- NULL # Create an empty table to store the weight of interaction hypotheses for (i in seq_len(nrow(pvals))) { # Each row in `pvals` is 1 possible interaction hypothesis   pval_tmp <- as.numeric(pvals[i, ])   graph_tmp <- gMCPLite::gMCP(graph = graph, pvalues = pval_tmp, alpha = alpha)   weight_tmp <- gMCPLite::getWeights(graph_tmp)   inter_weight <- dplyr::bind_rows(inter_weight, weight_tmp) }  inter_weight <- replace(inter_weight, pvals == 0, NA) # Replace the empty hypothesis as NA inter_weight <- inter_weight[-1, ] # Delete the first row since it is empty set  inter_weight %>%   gt() %>%   tab_header(\"Weight of all possible interaction hypothesis\")"},{"path":"https://merck.github.io/wpgsd/articles/tech-detail-wb.html","id":"correlations","dir":"Articles","previous_headings":"Example overview","what":"Correlations","title":"Procedure to compute p-value boundaries by weighted Bonferroni","text":"correlation 6 statistic (2 analyses ×\\times 3 hypotheses) ","code":"# Event count of intersection of paired hypotheses - Table 2 # H1, H2: Hypotheses intersected. # (1, 1) represents counts for hypothesis 1 # (1, 2) for counts for the intersection of hypotheses 1 and 2 event <- tribble(   ~H1, ~H2, ~Analysis, ~Event,   1, 1, 1, event_tbl %>% filter(analysis == 1, population == \"A positive\") %>% select(event) %>% as.numeric(),   2, 2, 1, event_tbl %>% filter(analysis == 1, population == \"B positive\") %>% select(event) %>% as.numeric(),   3, 3, 1, event_tbl %>% filter(analysis == 1, population == \"overall\") %>% select(event) %>% as.numeric(),   1, 2, 1, event_tbl %>% filter(analysis == 1, population == \"AB positive\") %>% select(event) %>% as.numeric(),   1, 3, 1, event_tbl %>% filter(analysis == 1, population == \"A positive\") %>% select(event) %>% as.numeric(),   2, 3, 1, event_tbl %>% filter(analysis == 1, population == \"B positive\") %>% select(event) %>% as.numeric(),   1, 1, 2, event_tbl %>% filter(analysis == 2, population == \"A positive\") %>% select(event) %>% as.numeric(),   2, 2, 2, event_tbl %>% filter(analysis == 2, population == \"B positive\") %>% select(event) %>% as.numeric(),   3, 3, 2, event_tbl %>% filter(analysis == 2, population == \"overall\") %>% select(event) %>% as.numeric(),   1, 2, 2, event_tbl %>% filter(analysis == 2, population == \"AB positive\") %>% select(event) %>% as.numeric(),   1, 3, 2, event_tbl %>% filter(analysis == 2, population == \"A positive\") %>% select(event) %>% as.numeric(),   2, 3, 2, event_tbl %>% filter(analysis == 2, population == \"B positive\") %>% select(event) %>% as.numeric() ) event ## # A tibble: 12 × 4 ##       H1    H2 Analysis Event ##    <dbl> <dbl>    <dbl> <dbl> ##  1     1     1        1    80 ##  2     2     2        1    88 ##  3     3     3        1   180 ##  4     1     2        1    64 ##  5     1     3        1    80 ##  6     2     3        1    88 ##  7     1     1        2   160 ##  8     2     2        2   176 ##  9     3     3        2   360 ## 10     1     2        2   128 ## 11     1     3        2   160 ## 12     2     3        2   176 # Generate correlation from events corr <- wpgsd::generate_corr(event) corr %>% round(2) ##      H1_A1 H2_A1 H3_A1 H1_A2 H2_A2 H3_A2 ## [1,]  1.00  0.76  0.67  0.71  0.54  0.47 ## [2,]  0.76  1.00  0.70  0.54  0.71  0.49 ## [3,]  0.67  0.70  1.00  0.47  0.49  0.71 ## [4,]  0.71  0.54  0.47  1.00  0.76  0.67 ## [5,]  0.54  0.71  0.49  0.76  1.00  0.70 ## [6,]  0.47  0.49  0.71  0.67  0.70  1.00"},{"path":[]},{"path":"https://merck.github.io/wpgsd/articles/tech-detail-wb.html","id":"boundary-of-h_1","dir":"Articles","previous_headings":"Boundary calculation","what":"Boundary of H1H_1","title":"Procedure to compute p-value boundaries by weighted Bonferroni","text":"elementary hypothesis H1H_1, weight 1, namely,","code":"w_H1 <- 1  # Index to select from the correlation matrix indx <- grep(\"H1\", colnames(corr)) corr_H1 <- corr[indx, indx]  # Boundary for a single hypothesis across k for the intersection hypothesis pval_H1 <- 1 - pnorm(gsDesign::gsDesign(   k = k,   test.type = 1,   usTime = IF_IA[1],   n.I = corr_H1[, ncol(corr_H1)]^2,   alpha = alpha * w_H1[1],   sfu = sfHSD,   sfupar = -4 )$upper$bound)  ans <- tibble(   Analysis = 1:2,   `Interaction/Elementary hypotheses` = \"H1\",   `H1 p-value boundary` = pval_H1,   `H2 p-value boundary` = NA,   `H3 p-value boundary` = NA ) ans %>% gt()"},{"path":"https://merck.github.io/wpgsd/articles/tech-detail-wb.html","id":"boundary-of-h_2","dir":"Articles","previous_headings":"Boundary calculation","what":"Boundary of H2H_2","title":"Procedure to compute p-value boundaries by weighted Bonferroni","text":"elementary hypothesis H2H_2, weight 1, namely,","code":"w_H2 <- 1  # Index to select from the correlation matrix indx <- grep(\"H2\", colnames(corr)) corr_H2 <- corr[indx, indx]  # Boundary for a single hypothesis across k for the intersection hypothesis pval_H2 <- 1 - pnorm(gsDesign::gsDesign(   k = k,   test.type = 1,   usTime = IF_IA[2],   n.I = corr_H2[, ncol(corr_H2)]^2,   alpha = alpha * w_H2[1],   sfu = sfHSD,   sfupar = -4 )$upper$bound)  ans_new <- tibble(   Analysis = 1:2,   `Interaction/Elementary hypotheses` = \"H2\",   `H1 p-value boundary` = NA,   `H2 p-value boundary` = pval_H2,   `H3 p-value boundary` = NA ) ans_new %>% gt() ans <- rbind(ans, ans_new)"},{"path":"https://merck.github.io/wpgsd/articles/tech-detail-wb.html","id":"boundary-of-h_3","dir":"Articles","previous_headings":"Boundary calculation","what":"Boundary of H3H_3","title":"Procedure to compute p-value boundaries by weighted Bonferroni","text":"elementary hypothesis H3H_3, weight 1, namely,","code":"w_H3 <- 1  # Index to select from the correlation matrix indx <- grep(\"H3\", colnames(corr)) corr_H3 <- corr[indx, indx]  # Boundary for a single hypothesis across k for the intersection hypothesis pval_H3 <- 1 - pnorm(gsDesign::gsDesign(   k = k,   test.type = 1,   usTime = IF_IA[3],   n.I = corr_H3[, ncol(corr_H3)]^2,   alpha = alpha * w_H3[1],   sfu = sfHSD,   sfupar = -4 )$upper$bound)  ans_new <- tibble(   Analysis = 1:2,   `Interaction/Elementary hypotheses` = \"H3\",   `H1 p-value boundary` = NA,   `H2 p-value boundary` = NA,   `H3 p-value boundary` = pval_H1 ) ans_new %>% gt() ans <- rbind(ans, ans_new)"},{"path":"https://merck.github.io/wpgsd/articles/tech-detail-wb.html","id":"boundary-of-h_1-cap-h_2","dir":"Articles","previous_headings":"Boundary calculation","what":"Boundary of H1∩H2H_1 \\cap H_2","title":"Procedure to compute p-value boundaries by weighted Bonferroni","text":"interaction hypothesis H1∩H2H_1 \\cap H_2, weight boundary H1H_1 H2H_2 ","code":"w_H12 <- inter_weight %>% filter(!is.na(H1), !is.na(H2), is.na(H3)) w_H12 <- w_H12[(!is.na(w_H12))] # Remove NA from weight w_H12 ## [1] 0.5 0.5 # -------------# #      H1      # # -------------# # Index to select from the correlation matrix indx <- grep(\"H1\", colnames(corr)) corr_H1 <- corr[indx, indx]  # Boundary for a single hypothesis across k for the intersection hypothesis pval_H1 <- 1 - pnorm(gsDesign::gsDesign(   k = k,   test.type = 1,   usTime = IF_IA[1],   n.I = corr_H1[, ncol(corr_H1)]^2,   alpha = alpha * w_H12[1], # alpha is different since the weight is updated   sfu = sfHSD,   sfupar = -4 )$upper$bound)  # -------------# #      H2      # # -------------# # Index to select from the correlation matrix indx <- grep(\"H2\", colnames(corr)) corr_H2 <- corr[indx, indx]  # Boundary for a single hypothesis across k for the intersection hypothesis pval_H2 <- 1 - pnorm(gsDesign::gsDesign(   k = k,   test.type = 1,   usTime = IF_IA[2],   n.I = corr_H2[, ncol(corr_H2)]^2,   alpha = alpha * w_H12[2], # alpha is different since the weight is updated   sfu = sfHSD,   sfupar = -4 )$upper$bound)  ans_new <- tibble(   Analysis = 1:2,   `Interaction/Elementary hypotheses` = \"H1, H2\",   `H1 p-value boundary` = pval_H1,   `H2 p-value boundary` = pval_H2,   `H3 p-value boundary` = NA ) ans_new %>% gt() ans <- rbind(ans, ans_new)"},{"path":"https://merck.github.io/wpgsd/articles/tech-detail-wb.html","id":"boundary-of-h_1-cap-h_3","dir":"Articles","previous_headings":"Boundary calculation","what":"Boundary of H1∩H3H_1 \\cap H_3","title":"Procedure to compute p-value boundaries by weighted Bonferroni","text":"interaction hypothesis H1∩H2H_1 \\cap H_2, weight boundary H1H_1 H3H_3 ","code":"w_H13 <- inter_weight %>% filter(!is.na(H1), is.na(H2), !is.na(H3)) w_H13 <- w_H13[(!is.na(w_H13))] # Remove NA from weight w_H13 ## [1] 0.4285714 0.5714286 # -------------# #      H1      # # -------------# # Index to select from the correlation matrix indx <- grep(\"H1\", colnames(corr)) corr_H1 <- corr[indx, indx]  # Boundary for a single hypothesis across k for the intersection hypothesis pval_H1 <- 1 - pnorm(gsDesign::gsDesign(   k = k,   test.type = 1,   usTime = IF_IA[1],   n.I = corr_H1[, ncol(corr_H1)]^2,   alpha = alpha * w_H13[1], # alpha is different since the weight is updated   sfu = sfHSD,   sfupar = -4 )$upper$bound)  # -------------# #      H3      # # -------------# # Index to select from the correlation matrix indx <- grep(\"H3\", colnames(corr)) corr_H3 <- corr[indx, indx]  # Boundary for a single hypothesis across k for the intersection hypothesis pval_H3 <- 1 - pnorm(gsDesign::gsDesign(   k = k,   test.type = 1,   usTime = IF_IA[3],   n.I = corr_H3[, ncol(corr_H3)]^2,   alpha = alpha * w_H13[2], # alpha is different since the weight is updated   sfu = sfHSD,   sfupar = -4 )$upper$bound)  ans_new <- tibble(   Analysis = 1:2,   `Interaction/Elementary hypotheses` = \"H1, H3\",   `H1 p-value boundary` = pval_H1,   `H2 p-value boundary` = NA,   `H3 p-value boundary` = pval_H3 ) ans_new %>% gt() ans <- rbind(ans, ans_new)"},{"path":"https://merck.github.io/wpgsd/articles/tech-detail-wb.html","id":"boundary-of-h_2-cap-h_3","dir":"Articles","previous_headings":"Boundary calculation","what":"Boundary of H2∩H3H_2 \\cap H_3","title":"Procedure to compute p-value boundaries by weighted Bonferroni","text":"interaction hypothesis H2∩H3H_2 \\cap H_3, weight boundary H2H_2 H3H_3 ","code":"w_H23 <- inter_weight %>% filter(is.na(H1), !is.na(H2), !is.na(H3)) w_H23 <- w_H23[(!is.na(w_H23))] # Remove NA from weight w_H23 ## [1] 0.4285714 0.5714286 # -------------# #      H2      # # -------------# # Index to select from the correlation matrix indx <- grep(\"H2\", colnames(corr)) corr_H2 <- corr[indx, indx]  # Boundary for a single hypothesis across k for the intersection hypothesis pval_H2 <- 1 - pnorm(gsDesign::gsDesign(   k = k,   test.type = 1,   usTime = IF_IA[2],   n.I = corr_H2[, ncol(corr_H2)]^2,   alpha = alpha * w_H23[1], # alpha is different since the weight is updated   sfu = sfHSD,   sfupar = -4 )$upper$bound)  # -------------# #      H3      # # -------------# # Index to select from the correlation matrix indx <- grep(\"H3\", colnames(corr)) corr_H3 <- corr[indx, indx]  # Boundary for a single hypothesis across k for the intersection hypothesis pval_H3 <- 1 - pnorm(gsDesign::gsDesign(   k = k,   test.type = 1,   usTime = IF_IA[3],   n.I = corr_H3[, ncol(corr_H3)]^2,   alpha = alpha * w_H23[2], # alpha is different since the weight is updated   sfu = sfHSD,   sfupar = -4 )$upper$bound)  ans_new <- tibble(   Analysis = 1:2,   `Interaction/Elementary hypotheses` = \"H2, H3\",   `H1 p-value boundary` = NA,   `H2 p-value boundary` = pval_H2,   `H3 p-value boundary` = pval_H3 ) ans_new %>% gt() ans <- rbind(ans, ans_new)"},{"path":"https://merck.github.io/wpgsd/articles/tech-detail-wb.html","id":"boundary-of-h1-cap-h_2-cap-h_3","dir":"Articles","previous_headings":"Boundary calculation","what":"Boundary of H1∩H2∩H3H1 \\cap H_2 \\cap H_3","title":"Procedure to compute p-value boundaries by weighted Bonferroni","text":"interaction hypothesis H1∩H2H_1 \\cap H_2, weight boundary H1H_1, H2H_2, H3H_3 ","code":"w_H123 <- inter_weight %>% filter(!is.na(H1), !is.na(H2), !is.na(H3)) w_H123 <- w_H123[(!is.na(w_H123))] # Remove NA from weight w_H123 ## [1] 0.3 0.3 0.4 # -------------# #      H1      # # -------------# # Index to select from the correlation matrix indx <- grep(\"H1\", colnames(corr)) corr_H1 <- corr[indx, indx]  # Boundary for a single hypothesis across k for the intersection hypothesis pval_H1 <- 1 - pnorm(gsDesign::gsDesign(   k = k,   test.type = 1,   usTime = IF_IA[1],   n.I = corr_H1[, ncol(corr_H1)]^2,   alpha = alpha * w_H123[1], # alpha is different since the weight is updated   sfu = sfHSD,   sfupar = -4 )$upper$bound)  # -------------# #      H2      # # -------------# # Index to select from the correlation matrix indx <- grep(\"H2\", colnames(corr)) corr_H2 <- corr[indx, indx]  # Boundary for a single hypothesis across k for the intersection hypothesis pval_H2 <- 1 - pnorm(gsDesign::gsDesign(   k = k,   test.type = 1,   usTime = IF_IA[2],   n.I = corr_H2[, ncol(corr_H2)]^2,   alpha = alpha * w_H123[1], # alpha is different since the weight is updated   sfu = sfHSD,   sfupar = -4 )$upper$bound)  # -------------# #      H3      # # -------------# # Index to select from the correlation matrix indx <- grep(\"H3\", colnames(corr)) corr_H3 <- corr[indx, indx]  # Boundary for a single hypothesis across k for the intersection hypothesis pval_H3 <- 1 - pnorm(gsDesign::gsDesign(   k = k,   test.type = 1,   usTime = IF_IA[3],   n.I = corr_H3[, ncol(corr_H3)]^2,   alpha = alpha * w_H123[3], # alpha is different since the weight is updated   sfu = sfHSD,   sfupar = -4 )$upper$bound)  ans_new <- tibble(   Analysis = 1:2,   `Interaction/Elementary hypotheses` = \"H1, H2, H3\",   `H1 p-value boundary` = pval_H1,   `H2 p-value boundary` = pval_H2,   `H3 p-value boundary` = pval_H3 ) ans_new %>% gt() ans <- rbind(ans, ans_new)"},{"path":"https://merck.github.io/wpgsd/articles/tech-detail-wb.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Procedure to compute p-value boundaries by weighted Bonferroni","text":"p-value boundaries, one can get Z-statistics boundaries qnorm().","code":"ans %>%   mutate(     `H1 Z-statistics boundary` = -qnorm(`H1 p-value boundary`),     `H1 Z-statistics boundary` = -qnorm(`H2 p-value boundary`),     `H1 Z-statistics boundary` = -qnorm(`H3 p-value boundary`)   ) %>%   arrange(Analysis, `Interaction/Elementary hypotheses`) %>%   gt() %>%   tab_header(\"p-values/Z-statistics boundaries of weighted Bonferroni\")"},{"path":"https://merck.github.io/wpgsd/articles/tech-detail-wb.html","id":"implementation-in-wpgsd","dir":"Articles","previous_headings":"","what":"Implementation in wpgsd","title":"Procedure to compute p-value boundaries by weighted Bonferroni","text":"results can computed one function call wpgsd using generate_bounds() function ","code":"generate_bounds(   type = 0,   k = 2,   w = w,   m = m,   corr = corr,   alpha = 0.025,   sf = list(sfHSD, sfHSD, sfHSD),   sfparm = list(-4, -4, -4),   t = list(c(0.5, 1), c(0.5, 1), c(0.5, 1)) ) %>% gt()"},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"background","dir":"Articles","previous_headings":"","what":"Background","title":"Quickstart guide","text":"weighted parametric group sequential design (WPGSD) (Anderson et al. (2022)) approach allows one take advantage known correlation structure constructing efficacy bounds control family-wise error rate (FWER) group sequential design. correlation may due common observations nested populations, due common observations overlapping populations, due common observations control arm. document illustrates use R package wpgsd implement approach.","code":""},{"path":[]},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"closed-testing-and-parametric-tests","dir":"Articles","previous_headings":"Methods and Examples","what":"Closed Testing and Parametric Tests","title":"Quickstart guide","text":"aim control familywise error rate (FWER) level α\\alpha. Let J⊆IJ \\subseteq . intersection hypothesis HJH_J assumes null hypothesis individual hypotheses HiH_i ∈Ji \\J. Closed testing principle follows: sets J⊆IJ \\subseteq j∈Jj \\J, HJH_J can rejected level α\\alpha, HjH_j can rejected. Weighted parametric tests can used : Bretz et al. (2011), Xi et al. (2017) fixed designs Maurer Bretz (2013) group sequential.","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"consonance","dir":"Articles","previous_headings":"Methods and Examples","what":"Consonance","title":"Quickstart guide","text":"closed procedure called consonant rejection complete intersection null hypothesis HIH_I implies least one elementary hypothesis Hi,∈IH_i, \\, rejected. Consonance desirable property leading short-cut procedures give rejection decisions original closed procedure fewer operations. WPGSD, consonance always hold general closed-testing procedure required.","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"group-sequential-design-notations-and-assumptions","dir":"Articles","previous_headings":"Methods and Examples","what":"Group Sequential Design Notations and Assumptions","title":"Quickstart guide","text":"set II hypotheses ∈Ii \\. KK group sequential analyses, k=1,…,Kk = 1, \\ldots, K required, can generalized Assume tests ZikZ_{ik}, ∈Ii \\, 1≤k≤K1 \\leq k \\leq K large ZikZ_{ik} used reject HiH_i","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"correlation-structure","dir":"Articles","previous_headings":"Methods and Examples","what":"Correlation Structure","title":"Quickstart guide","text":"Events individual hypothesis HiH_i,∈Ii \\analysis k denoted nikn_{ik}. Assume endpoint hypotheses (can relaxed) binary continuous outcomes nikn_{ik} represents sample size ZikZ_{ik} standardized normal test treatment effect individual hypothesis HiH_i analysis kk Denote ni∧′,k∧k′n_{\\wedge ^\\prime,k\\wedge k^\\prime} number observations (events) included ZikZ_{ik} Zi′k′Z_{^\\prime k^\\prime}, ∈Ii\\, 1≤k≤K1\\le k\\le K. Key result $$  \\hbox{Corr}(Z_{ik}, Z_{^\\prime k^\\prime }) =  \\frac{n_{\\wedge ^\\prime ,k\\wedge k^\\prime }}{\\sqrt{n_{ik}n_{^\\prime k^\\prime }}} $$ Proof builds standard group sequential theory (Chen et al. (2021))","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"example-1-overlapping-populations-two-analyses","dir":"Articles","previous_headings":"Methods and Examples","what":"Example 1: Overlapping Populations, Two Analyses","title":"Quickstart guide","text":"Following illustrates first example, overlapping populations (e.g. due biomarker) also example 1 Anderson et al. (2022). Ex1: Populations multiplicity strategy defined follows.  event count hypothesis analysis shown . Number events analysis population Example 1. IA: interim analysis. FA: final analysis. correlation matrix among test statistics follows. Correlation Matrix Test Statistics Example 1. Identical numeric values (lower triangular) formulas (upper triangular) shown.","code":"# Transition matrix m <- matrix(c(   0, 0, 1,   0, 0, 1,   0.5, 0.5, 0 ), nrow = 3, byrow = TRUE) # Weight matrix w <- c(0.3, 0.3, 0.4)  # Multiplicity graph cbPalette <- c(\"#999999\", \"#E69F00\", \"#56B4E9\")  nameHypotheses <- c(   \"H1: Population 1\",   \"H2: Population 2\",   \"H3: Overall Population\" )  hplot <- hGraph(3,   alphaHypotheses = w,   m = m,   nameHypotheses = nameHypotheses,   trhw = .2, trhh = .1,   digits = 5, trdigits = 3, size = 5, halfWid = 1,   halfHgt = 0.5, offset = 0.2, trprop = 0.4,   fill = as.factor(c(2, 3, 1)),   palette = cbPalette,   wchar = \"w\" ) hplot"},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"example-2-common-control-two-analyses","dir":"Articles","previous_headings":"Methods and Examples","what":"Example 2: Common Control, Two Analyses","title":"Quickstart guide","text":"Following illustrates second example correlation comes common control arm. also example 2 Anderson et al. (2022).  Number events analysis treatment arm Example 2. IA: interim analysis. FA: final analysis. Correlation Matrix Example 2. Identical numeric values (lower triangular) formulas (upper triangular) shown.","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"hypotheses-set","dir":"Articles","previous_headings":"Methods and Examples","what":"Hypotheses Set","title":"Quickstart guide","text":"2 examples 7 intersection hypotheses corresponding weighting strategies illustrated . Weighting strategy Example 1. Weighting strategy Example 2.","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"alpha-spending-3-approaches","dir":"Articles","previous_headings":"Methods and Examples","what":"α\\alpha Spending: 3 approaches","title":"Quickstart guide","text":"WPGSD approach uses known correlations tests study. relaxes bounds allows increased power smaller sample size. Three spending approaches proposed: Fixed spending (Fleming-Harrington-O’Brien (FHO) approach). Specify 0<α1(J)<α2(J)<…<αK(J)=α(J)≤α0 < \\alpha_1(J) < \\alpha_2(J) < \\ldots < \\alpha_K(J) = \\alpha(J) \\leq \\alpha J⊆IJ\\subseteq , α(J)\\alpha(J) total alpha intersection hypothesis HJH_J according graphical approach. α\\alpha-spending approach 1. choose spending function family f(t,α)f(t,\\alpha) set αk(J)=f(tk(J),α(J))\\alpha_k(J)=f(t_k(J),\\alpha(J)) 1≤k≤K1\\le k\\le K intersection hypotheses J⊆IJ\\subseteq . α\\alpha-spending approach 2. elementary hypothesis ii (ii = 1, 2, , mm), specify α\\alpha-spending function family fi(t,γ)f_i(t,\\gamma) γ\\gamma α\\alpha level hypothesis fi(tik,γ)f_i(t_{ik},\\gamma) determines much α\\alpha spend analysis kk hypothesis ii level γ\\gamma allocated hypothesis. αk(J)=∑∈Jfi(tik,wi(J)α)\\alpha_k(J) = \\sum_{\\J} f_i(t_{ik}, w_i(J)\\alpha).","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"bounds-computation-parametric-test-fixed-design-for-example-two-populations-one-analysis","dir":"Articles","previous_headings":"Methods and Examples","what":"Bounds Computation: Parametric Test, Fixed Design (For Example, Two Populations, One Analysis)","title":"Quickstart guide","text":"Assume (Z1,Z2Z_1,Z_2) bivariate normal known correlation Find α\\alpha-inflation factor cJc_J α=P[∪∈J{pi≤cJwJ,iα}]=P[∪∈J{Zi≥Φ−1(1−cJwJ,iα}] \\alpha = P[\\cup_{\\J} \\{p_i \\leq c_Jw_{J,}\\alpha \\}] = P[\\cup_{\\J} \\{Z_i \\geq \\Phi^{-1}(1-c_Jw_{J,}\\alpha \\}] Basic algorithm code Bretz et al. (2011)","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"bounds-computation-wpgsd---fixed-spending-and-alpha-spending-approach-1","dir":"Articles","previous_headings":"Methods and Examples > Bounds Computation: Parametric Test, Fixed Design (For Example, Two Populations, One Analysis)","what":"Bounds Computation: WPGSD - Fixed spending and α\\alpha spending approach 1","title":"Quickstart guide","text":"Assume j<kj < k bounds cij(J),∈J,j<kc_{ij} (J), \\J, j < k, already set remain unchanged. analysis kk, compute correlation matrix ZijZ_{ij}, ∈Ji \\J, j=1,…,kj = 1, \\ldots, k. Initialize αk*(J)=αk(J)−αk−1(J)\\alpha_{k}^{*}(J) = \\alpha_{k}(J) - \\alpha_{k-1}(J). ii Set bik=Φ−1(1−wi(J)αk*(J))b_{ik} = \\Phi^{-1}(1 - w_{}(J)\\alpha_{k}^{*} (J)), ∈Ji\\J. iii Compute type error rate analysis kk1−Pr(∩∈J{Zik<bik}∩∈J,j<k{Zij<cij(J)}). 1 - Pr(\\cap_{\\J} \\{ Z_{ik} < b_{ik} \\} \\cap_{\\J, j < k} \\{ Z_{ij} < c_{ij}(J) \\} ). iv Update αk*(J)\\alpha_{k}^{*}(J) using root-finding steps ii - iii type error rate analysis kk controlled αk(J)\\alpha_{k}(J) HJH_J. , 1−Pr(∩∈J{Zik<bik}∩∈J,j<k{Zij<cij(J)})=αk. 1 - Pr(\\cap_{\\J} \\{ Z_{ik} < b_{ik} \\} \\cap_{\\J, j < k} \\{ Z_{ij} < c_{ij}(J) \\} ) = \\alpha_{k}. v Set cik(J)=bikc_{ik}(J) = b_{ik} previous step. corresponding nominal pp-value boundary pik(J)=1−Φ(cik(J))=wi(J)αk*(J)p_{ik}(J)= 1-\\Phi(c_{ik}(J)) =  w_i(J)\\alpha_k^*(J). Note: interim bound depend future analyses. Solution requires root finding single αk*(J)\\alpha_{k}^{*}(J) time, k=1,…,Kk = 1, \\ldots, K. Requires multivariate normal computation mvtnorm R package Genz et al. (2020).","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"bounds-computation-wpgsd---alpha-spending-approach-2","dir":"Articles","previous_headings":"Methods and Examples > Bounds Computation: Parametric Test, Fixed Design (For Example, Two Populations, One Analysis)","what":"Bounds Computation: WPGSD - α\\alpha spending approach 2","title":"Quickstart guide","text":"Assume j<kj < k bounds cij(J),∈J,j<kc_{ij} (J), \\J, j < k, already set remain unchanged. analysis kk, compute correlation matrix ZijZ_{ij}, ∈Ji \\J, j=1,…,kj = 1, \\ldots, k. Determine nominal pp-value boundary elementary hypothesis JJ weighted Bonferroni test group sequential design described Maurer Bretz (2013). Let nominal pp-value boundaries αik′(J)\\alpha^\\prime_{ik}(J). ii Choose inflation factor ξk(J)>1\\xi_{k}(J) > 1 set bik=Φ−1(1−ξk(J)αik′(J)).b_{ik} = \\Phi^{-1}(1 - \\xi_k(J) \\alpha^\\prime_{ik}(J)). iii Update ξk(J)\\xi_k(J) type error rate analysis kk controlled αk(J)\\alpha_{k}(J) HJH_J. , 1−Pr(∩∈J{Zik<bik}∩∈J,j<k{Zij<cij(J)})=αk(J). 1 - Pr(\\cap_{\\J} \\{ Z_{ik} < b_{ik} \\} \\cap_{\\J, j < k} \\{ Z_{ij} < c_{ij}(J) \\} ) = \\alpha_{k}(J). iv appropriate ξk(J)\\xi_k(J) derived, nominal pp-value boundaries pik(J)=ξk(J)αik′(J)p_{ik}(J)=\\xi_k(J) \\alpha^\\prime_{ik}(J), bikb_{ik} computed step ii, set cik(J)=bikc_{ik}(J) = b_{ik}. Note: interim bound depend future analyses. Solution requires root finding single ξk(J)\\xi_k(J) time, k=1,…,Kk = 1, \\ldots, K. Requires multivariate normal computation mvtnorm R package Genz et al. (2020).","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"implementation-of-example-1-with-overlapping-populations","dir":"Articles","previous_headings":"Methods and Examples","what":"Implementation of Example 1 with Overlapping Populations","title":"Quickstart guide","text":"first define transition matrix weights shown Section 2.5. Next set event count table follows: Analysis: Analysis number (1 interim, 2 final). Event: Event counts. (1, 1) represents counts hypothesis 1 (1, 2) counts intersection hypotheses 1 2 compute correlation matrix using event count table generate_corr(). see correlations accounted Bonferroni approach substantial , thus, might expect non-trivial impact bounds hypothesis tests. Bonferroni WPGSD bounds can computed via generate_bounds(). example, useHSD(-4) α\\alpha-spending hypotheses. note, generate_bounds() input type specifies boundary type. 0 = Bonferroni. Separate alpha spending hypotheses. 1 = Fixed alpha spending hypotheses. Method 3a manuscript. 2 = Overall alpha spending hypotheses. Method 3b manuscript. 3 = Separate alpha spending hypotheses. Method 3c manuscript. Compute Bonferroni bounds. Compute WPGSD Bounds using α\\alpha-spending approach 1 HSD(-4) spending. spending time defined minimum 3 observed information fractions. shows comparison Bonferroni WPGSD bounds. Nominal level final analysis using WPGSD method increased 1.3× obtained via Bonferroni approach. Closed testing procedure can performed using closed_test().","code":"event <- tribble(   ~H1, ~H2, ~Analysis, ~Event,   1, 1, 1, 100,   2, 2, 1, 110,   3, 3, 1, 225,   1, 2, 1, 80,   1, 3, 1, 100,   2, 3, 1, 110,   1, 1, 2, 200,   2, 2, 2, 220,   3, 3, 2, 450,   1, 2, 2, 160,   1, 3, 2, 200,   2, 3, 2, 220 ) event %>%   gt() %>%   tab_header(title = \"Event Count\") # Alternatively, one can manually enter paths for analysis datasets, # example below uses an example dataset assuming currently we are at IA1. paths <- system.file(\"extdata/\", package = \"wpgsd\")  ### Generate event count table from ADSL and ADTTE datasets # Selection criteria for each hypothesis h_select <- tribble(   ~Hypothesis, ~Crit,   1, \"PARAMCD=='OS' & TRT01P %in% c('Xanomeline High Dose', 'Placebo')\",   2, \"PARAMCD=='OS' & TRT01P %in% c('Xanomeline Low Dose', 'Placebo')\" )  event2 <- generate_event_table(paths, h_select,   adsl_name = \"adsl\", adtte_name = \"adtte\",   key_var = \"USUBJID\", cnsr_var = \"CNSR\" )$event  event2 %>%   gt() %>%   tab_header(title = \"Event Count - Computed from SAS Datasets Example\") ## Generate correlation from events corr <- generate_corr(event)  corr %>%   as_tibble() %>%   gt() %>%   fmt_number(columns = everything(), decimals = 2) %>%   tab_header(title = \"Correlation Matrix\") # Bonferroni bounds bound_Bonf <- generate_bounds(   type = 0, k = 2, w = w, m = m,   corr = corr, alpha = 0.025,   sf = list(sfHSD, sfHSD, sfHSD),   sfparm = list(-4, -4, -4),   t = list(c(0.5, 1), c(0.5, 1), c(0.5, 1)) )  bound_Bonf %>%   gt() %>%   fmt_number(columns = 3:5, decimals = 4) %>%   tab_header(title = \"Bonferroni bounds\") set.seed(1234) # WPGSD bounds, spending approach 1 bound_WPGSD <- generate_bounds(   type = 2, k = 2, w = w, m = m,   corr = corr, alpha = 0.025,   sf = sfHSD,   sfparm = -4,   t = c(min(100 / 200, 110 / 220, 225 / 450), 1) )  bound_WPGSD %>%   gt() %>%   fmt_number(columns = 3:5, decimals = 4) %>%   tab_header(title = \"WPGSD bounds\") ## Observed p-values. ## The tibble must contain columns Analysis, H1, H2 etc for all hypotheses p_obs <- tribble(   ~Analysis, ~H1, ~H2, ~H3,   1, 0.01, 0.0004, 0.03,   2, 0.05, 0.002, 0.015 )  ## Closed testing ## test_result <- closed_test(bound_WPGSD, p_obs)  p_obs %>%   gt() %>%   fmt_number(columns = 2:4, decimals = 8, drop_trailing_zeros = TRUE) %>%   tab_header(\"Observed Nominal p-Values\") test_result %>%   gt() %>%   tab_header(title = \"Closed Testing Results\")"},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"implementation-of-example-2-with-common-control","dir":"Articles","previous_headings":"Methods and Examples","what":"Implementation of Example 2 with Common Control","title":"Quickstart guide","text":"Similarly, codes reproduce result Example 2 Anderson et al. (2022), uses spending method 3c specified paper.","code":"set.seed(1234)  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Ex2 BH ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# # Transition matrix in Figure A2 m <- matrix(c(   0, 0.5, 0.5,   0.5, 0, 0.5,   0.5, 0.5, 0 ), nrow = 3, byrow = TRUE) # Initial weights w <- c(1 / 3, 1 / 3, 1 / 3)  # Event count of intersection of paired hypotheses - Table 2 event <- tribble(   ~H1, ~H2, ~Analysis, ~Event,   1, 1, 1, 155,   2, 2, 1, 160,   3, 3, 1, 165,   1, 2, 1, 85,   1, 3, 1, 85,   2, 3, 1, 85,   1, 1, 2, 305,   2, 2, 2, 320,   3, 3, 2, 335,   1, 2, 2, 170,   1, 3, 2, 170,   2, 3, 2, 170 )  event %>%   gt() %>%   tab_header(title = \"Event Count\") # Generate correlation from events corr <- generate_corr(event)  # Correlation matrix in Table 4 corr %>%   as_tibble() %>%   gt() %>%   fmt_number(columns = everything(), decimals = 2) %>%   tab_header(title = \"Correlation Matrix\") # WPGSD bounds, spending method 3c bound_WPGSD <- generate_bounds(   type = 3, k = 2, w = w, m = m, corr = corr, alpha = 0.025,   sf = list(sfLDOF, sfLDOF, sfLDOF),   sfparm = list(0, 0, 0),   t = list(c(155 / 305, 1), c(160 / 320, 1), c(165 / 335, 1)) )  # Bonferroni bounds bound_Bonf <- generate_bounds(   type = 0, k = 2, w = w, m = m, corr = corr, alpha = 0.025,   sf = list(sfLDOF, sfLDOF, sfLDOF),   sfparm = list(0, 0, 0),   t = list(c(155 / 305, 1), c(160 / 320, 1), c(165 / 335, 1)) )  bounds <- left_join(bound_Bonf, bound_WPGSD,   by = c(\"Hypotheses\", \"Analysis\"),   suffix = c(\".B\", \".W\") )  # Reorder for output bounds$order <- rep(c(5, 2, 1, 3, 6, 4, 7), 2) bounds <- bounds %>%   arrange(Analysis, order) %>%   select(-order)  # Table A6 bounds %>%   gt() %>%   fmt_number(columns = 3:9, decimals = 4) %>%   tab_header(title = \"Bonferroni and WPGSD Bounds\")"},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"power-considerations","dir":"Articles","previous_headings":"Methods and Examples","what":"Power Considerations","title":"Quickstart guide","text":"illustrates use WPGSD approach compute bounds analysis stage. design stage, one can take one following 2 options: 1) trial can first designed testing done weighted Bonferroni conservative sample size estimate. analysis stage, correlation can taken consideration WPGSD approach bound calculation; 2) adjust sample size downward using WPGSD approach design stage, one can power study taking minimum pp-value bound given individual hypothesis WPGSD table (assumed correlation structure). example, H2H_2 example 1, $\\hbox{min}(0.0011,0.0017,0.0010,0.0030)=0.0010$ k=1k=1 $\\hbox{min}(0.0092,0.0144,0.0081,0.0238)=0.0081$ k=2k=2. H2H_2 bounds 0.0010 (k=1k=1) 0.0081 (k=2k=2) can used power H2H_2. R function 2nd option development.","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"Quickstart guide","text":"WPGSD approach provides unification previous work parametric testing group sequential design. enabled complex scenarios requires attention consonance intersection hypotheses. Although detailed closed testing required, deterrent. approach accommodates various spending approaches provides relaxed bounds improved power compared Bonferroni approach.","code":""},{"path":[]},{"path":"https://merck.github.io/wpgsd/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Keaven Anderson. Author. Zifang Guo. Author. Jing Zhao. Author. Linda Sun. Author. Yi Cui. Author. Yujie Zhao. Author, maintainer. Larry Leon. Author. Merck Sharp & Dohme Corp. Copyright holder.","code":""},{"path":"https://merck.github.io/wpgsd/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Anderson KM, Guo Z, Zhao J, Sun LZ (2022). “unified framework weighted parametric group sequential design.” Biometrical Journal, 64(7), 1219–1239. doi:10.1002/bimj.202100085.","code":"@Article{,   title = {A unified framework for weighted parametric group sequential design},   author = {Keaven M Anderson and Zifang Guo and Jing Zhao and Linda Z Sun},   journal = {Biometrical Journal},   volume = {64},   number = {7},   pages = {1219--1239},   year = {2022},   publisher = {Wiley Online Library},   doi = {10.1002/bimj.202100085}, }"},{"path":"https://merck.github.io/wpgsd/index.html","id":"wpgsd-","dir":"","previous_headings":"","what":"Weighted Parametric Group Sequential Design","title":"Weighted Parametric Group Sequential Design","text":"Weighted parametric group sequential design (WPGSD) allows one take advantage known correlation structure constructing efficacy bounds control family-wise error rate (FWER) group sequential design. correlation may due common observations nested populations, due common observations overlapping populations, due common observations control arm.","code":""},{"path":"https://merck.github.io/wpgsd/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Weighted Parametric Group Sequential Design","text":"easiest way get wpgsd install CRAN: Alternatively, use new feature get bug fix, can install development version wpgsd GitHub:","code":"install.packages(\"wpgsd\") # install.packages(\"remotes\") remotes::install_github(\"Merck/wpgsd\")"},{"path":"https://merck.github.io/wpgsd/index.html","id":"current-limitations","dir":"","previous_headings":"","what":"Current limitations","title":"Weighted Parametric Group Sequential Design","text":"limitations currently addressed. Please use package caution production environments. current implementation may limitations handling complex scenarios beyond demonstrated. API subject potential breaking changes currently reviewed refactored. validation needed ensure reliability package. documentation expected future releases.","code":""},{"path":"https://merck.github.io/wpgsd/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Weighted Parametric Group Sequential Design","text":"Anderson, K. M., Guo, Z., Zhao, J., & Sun, L. Z. (2022). unified framework weighted parametric group sequential design. Biometrical Journal, 64(7), 1219–1239. BibTeX entry:","code":"@article{anderson2022unified,   title     = {A unified framework for weighted parametric group sequential design},   author    = {Anderson, Keaven M and Guo, Zifang and Zhao, Jing and Sun, Linda Z},   journal   = {Biometrical Journal},   volume    = {64},   number    = {7},   pages     = {1219--1239},   year      = {2022},   publisher = {Wiley Online Library} }"},{"path":"https://merck.github.io/wpgsd/reference/calc_seq_p.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate sequential p-values for interaction/elementary hypothesis — calc_seq_p","title":"Calculate sequential p-values for interaction/elementary hypothesis — calc_seq_p","text":"Calculate sequential p-values interaction/elementary hypothesis","code":""},{"path":"https://merck.github.io/wpgsd/reference/calc_seq_p.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate sequential p-values for interaction/elementary hypothesis — calc_seq_p","text":"","code":"calc_seq_p(   test_analysis = 2,   test_hypothesis = \"H1, H2, H3\",   p_obs = tibble::tibble(analysis = 1:2, H1 = c(0.02, 0.0015), H2 = c(0.01, 0.01), H3 =     c(0.01, 0.004)),   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = c(0.3, 0.3, 0.4),   transition_mat = matrix(c(0, 0.4285714, 0.5714286, 0.4285714, 0, 0.5714286, 0.5, 0.5,     0), nrow = 3, byrow = TRUE),   z_corr = matrix(c(1, 0.7627701, 0.6666667, 0.7071068, 0.5393599, 0.4714045, 0.7627701,     1, 0.6992059, 0.5393599, 0.7071068, 0.4944132, 0.6666667, 0.6992059, 1, 0.4714045,     0.4944132, 0.7071068, 0.7071068, 0.5393599, 0.4714045, 1, 0.7627701, 0.6666667,     0.5393599, 0.7071068, 0.4944132, 0.7627701, 1, 0.6992059, 0.4714045, 0.4944132,     0.7071068, 0.6666667, 0.6992059, 1), nrow = 6, byrow = TRUE),   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(0.5, 1),   interval = c(1e-04, 0.2) )"},{"path":"https://merck.github.io/wpgsd/reference/calc_seq_p.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate sequential p-values for interaction/elementary hypothesis — calc_seq_p","text":"test_analysis index analysis tested, 1, 2, ... test_hypothesis character tested interaction/elementary hypothesis, \"H1, H2, H3\", H1, H2, \"H1\". p_obs Observed p-values test_analysis. alpha_spending_type Type Boundary type. 0 - Bonferroni. Separate alpha spending hypotheses. 1 - Fixed alpha spending hypotheses. Method 3a manuscript. 2 - Overall alpha spending hypotheses. Method 3b manuscript. 3 - Separate alpha spending hypotheses. Method 3c manuscript. n_analysis Total number analysis. initial_weight Initial weight assigned elementary hypothesis. transition_mat Transition matrix. z_corr Correlation matrix Z statistics. spending_fun Spending function. spending_fun_par Parameter spending function. info_frac Information fractions. interval Interval search uniroot.","code":""},{"path":"https://merck.github.io/wpgsd/reference/calc_seq_p.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate sequential p-values for interaction/elementary hypothesis — calc_seq_p","text":"sequential p-values test_hypothesis test_analysis.","code":""},{"path":"https://merck.github.io/wpgsd/reference/calc_seq_p.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate sequential p-values for interaction/elementary hypothesis — calc_seq_p","text":"","code":"# \\donttest{ calc_seq_p(   test_analysis = 2,   test_hypothesis = \"H1, H2, H3\",   p_obs = tibble::tibble(     analysis = 1:2,     H1 = c(0.02, 0.0015),     H2 = c(0.01, 0.01),     H3 = c(0.01, 0.004)   ),   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = c(0.3, 0.3, 0.4),   transition_mat = matrix(c(     0.0000000, 0.4285714, 0.5714286,     0.4285714, 0.0000000, 0.5714286,     0.5000000, 0.5000000, 0.0000000   ), nrow = 3, byrow = TRUE),   z_corr = matrix(     c(       1.0000000, 0.7627701, 0.6666667, 0.7071068, 0.5393599, 0.4714045,       0.7627701, 1.0000000, 0.6992059, 0.5393599, 0.7071068, 0.4944132,       0.6666667, 0.6992059, 1.0000000, 0.4714045, 0.4944132, 0.7071068,       0.7071068, 0.5393599, 0.4714045, 1.0000000, 0.7627701, 0.6666667,       0.5393599, 0.7071068, 0.4944132, 0.7627701, 1.0000000, 0.6992059,       0.4714045, 0.4944132, 0.7071068, 0.6666667, 0.6992059, 1.0000000     ),     nrow = 6, byrow = TRUE   ),   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(0.5, 1),   interval = c(1e-4, 0.2) )# } #> [1] 0.004514193"},{"path":"https://merck.github.io/wpgsd/reference/closed_test.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform closed testing procedure — closed_test","title":"Perform closed testing procedure — closed_test","text":"Perform closed testing procedure","code":""},{"path":"https://merck.github.io/wpgsd/reference/closed_test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform closed testing procedure — closed_test","text":"","code":"closed_test(bounds, p_obs)"},{"path":"https://merck.github.io/wpgsd/reference/closed_test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform closed testing procedure — closed_test","text":"bounds tibble nominal p-value boundaries generate_bounds() containing columns Analysis, Hypotheses, H1, H2, etc. p_obs tibble observed p-values containing columns Analysis, H1, H2, etc.","code":""},{"path":"https://merck.github.io/wpgsd/reference/closed_test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform closed testing procedure — closed_test","text":"outcome matrix summarizing testing results.","code":""},{"path":"https://merck.github.io/wpgsd/reference/closed_test.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform closed testing procedure — closed_test","text":"","code":"p_obs <- dplyr::bind_rows(   tibble::tibble(Analysis = 1, H1 = 0.001, H2 = 0.001),   tibble::tibble(Analysis = 2, H1 = 0.001, H2 = 0.001) ) bound <- tibble::tribble(   ~Analysis, ~Hypotheses, ~H1, ~H2,   1, \"H1\", 0.02, NA,   1, \"H1, H2\", 0.0001, 0.00001,   1, \"H2\", NA, 0.003,   2, \"H1\", 0.02, NA,   2, \"H1, H2\", 0.02, 0.00001,   2, \"H2\", NA, 0.003 )  closed_test <- closed_test(bound, p_obs)"},{"path":"https://merck.github.io/wpgsd/reference/find_astar.html","id":null,"dir":"Reference","previous_headings":"","what":"Utility function for root-finding to compute crossing probabilities with the overall alpha spending approach — find_astar","title":"Utility function for root-finding to compute crossing probabilities with the overall alpha spending approach — find_astar","text":"Utility function root-finding compute crossing probabilities overall alpha spending approach","code":""},{"path":"https://merck.github.io/wpgsd/reference/find_astar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Utility function for root-finding to compute crossing probabilities with the overall alpha spending approach — find_astar","text":"","code":"find_astar(   a,   alpha_prev = NULL,   astar,   w,   sig,   maxpts = 50000,   abseps = 1e-05,   ... )"},{"path":"https://merck.github.io/wpgsd/reference/find_astar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Utility function for root-finding to compute crossing probabilities with the overall alpha spending approach — find_astar","text":"Cumulative overall alpha spending current analysis. alpha_prev alpha boundary previous interim analyses using WPGSD approach. astar Total nominal alpha level current analysis WPGSD approach. w Vector alpha weights current analysis. sig Correlation matrix previous current analyses test statistics. maxpts GenzBretz function maximum number function values integer. abseps GenzBretz function absolute error tolerance. ... Additional arguments.","code":""},{"path":"https://merck.github.io/wpgsd/reference/find_astar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Utility function for root-finding to compute crossing probabilities with the overall alpha spending approach — find_astar","text":"Difference. 0 astar identified.","code":""},{"path":"https://merck.github.io/wpgsd/reference/find_astar.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Utility function for root-finding to compute crossing probabilities with the overall alpha spending approach — find_astar","text":"","code":"# Input event count of intersection of paired hypotheses - Table 2 my_event <- tibble::tribble(   ~H1, ~H2, ~Analysis, ~Event,   1, 1, 1, 155,   2, 2, 1, 160,   3, 3, 1, 165,   1, 2, 1, 85,   1, 3, 1, 85,   2, 3, 1, 85,   1, 1, 2, 305,   2, 2, 2, 320,   3, 3, 2, 335,   1, 2, 2, 170,   1, 3, 2, 170,   2, 3, 2, 170 )  # Generate correlation from events my_corr <- generate_corr(my_event)  # Find the inflation factor for H1, H2 at analysis 1 find_astar(   a = 0.0008708433,   alpha_prev = NULL,   aprime = c(0.0004588644, 0.0004119789),   astar = 1,   w = c(0.5, 0.5),   sig = my_corr[     colnames(my_corr) %in% c(\"H1_A1\", \"H2_A1\"),     colnames(my_corr) %in% c(\"H1_A1\", \"H2_A1\")   ] ) #> [1] 0.6583884 #> attr(,\"error\") #> [1] 1e-15 #> attr(,\"msg\") #> [1] \"Normal Completion\""},{"path":"https://merck.github.io/wpgsd/reference/find_xi.html","id":null,"dir":"Reference","previous_headings":"","what":"Utility function for root-finding to compute inflation factor xi with the separate alpha spending approach — find_xi","title":"Utility function for root-finding to compute inflation factor xi with the separate alpha spending approach — find_xi","text":"Utility function root-finding compute inflation factor xi separate alpha spending approach","code":""},{"path":"https://merck.github.io/wpgsd/reference/find_xi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Utility function for root-finding to compute inflation factor xi with the separate alpha spending approach — find_xi","text":"","code":"find_xi(   a,   alpha_prev = NULL,   aprime,   xi,   sig,   maxpts = 50000,   abseps = 1e-05,   ... )"},{"path":"https://merck.github.io/wpgsd/reference/find_xi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Utility function for root-finding to compute inflation factor xi with the separate alpha spending approach — find_xi","text":"Sum cumulative alpha spending Bonferroni approach. alpha_prev alpha boundary previous interim analyses using MTP approach. aprime Nominal alpha boundary Bonferroni approach. xi Inflation factor. sig Correlation matrix previous current analyses test statistics. maxpts GenzBretz function maximum number function values integer. abseps GenzBretz function absolute error tolerance. ... Additional arguments.","code":""},{"path":"https://merck.github.io/wpgsd/reference/find_xi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Utility function for root-finding to compute inflation factor xi with the separate alpha spending approach — find_xi","text":"Difference. 0 xi identified.","code":""},{"path":"https://merck.github.io/wpgsd/reference/find_xi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Utility function for root-finding to compute inflation factor xi with the separate alpha spending approach — find_xi","text":"","code":"# Input event count of intersection of paired hypotheses - Table 2 my_event <- tibble::tribble(   ~H1, ~H2, ~Analysis, ~Event,   1, 1, 1, 155,   2, 2, 1, 160,   3, 3, 1, 165,   1, 2, 1, 85,   1, 3, 1, 85,   2, 3, 1, 85,   1, 1, 2, 305,   2, 2, 2, 320,   3, 3, 2, 335,   1, 2, 2, 170,   1, 3, 2, 170,   2, 3, 2, 170 )  # Generate correlation from events my_corr <- generate_corr(my_event)  # Find the inflation factor for H1, H2 at analysis 1 find_xi(   a = 0.0008708433,   alpha_prev = NULL,   aprime = c(0.0004588644, 0.0004119789),   xi = 1,   sig = my_corr[     colnames(my_corr) %in% c(\"H1_A1\", \"H2_A1\"),     colnames(my_corr) %in% c(\"H1_A1\", \"H2_A1\")   ] ) #> [1] -2.237679e-05 #> attr(,\"error\") #> [1] 1e-15 #> attr(,\"msg\") #> [1] \"Normal Completion\""},{"path":"https://merck.github.io/wpgsd/reference/generate_bounds.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute p-value boundaries of the parametric MTP method with overall alpha spending for all hypotheses — generate_bounds","title":"Compute p-value boundaries of the parametric MTP method with overall alpha spending for all hypotheses — generate_bounds","text":"Compute p-value boundaries parametric MTP method overall alpha spending hypotheses","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_bounds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute p-value boundaries of the parametric MTP method with overall alpha spending for all hypotheses — generate_bounds","text":"","code":"generate_bounds(   type = 1,   k = 2,   w = w,   m = m,   corr = corr,   alpha = 0.025,   cum_alpha = NULL,   maxpts = 50000,   abseps = 1e-05,   tol = 1e-10,   sf = gsDesign::sfHSD,   sfparm = -4,   t = c(0.5, 1),   ... )"},{"path":"https://merck.github.io/wpgsd/reference/generate_bounds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute p-value boundaries of the parametric MTP method with overall alpha spending for all hypotheses — generate_bounds","text":"type Boundary type. 0 = Bonferroni. Separate alpha spending hypotheses. 1 = Fixed alpha spending hypotheses. Method 3a manuscript. 2 = Overall alpha spending hypotheses. Method 3b manuscript. 3 = Separate alpha spending hypotheses. Method 3c manuscript. k Number analyses current analysis. w Initial weights. m Transition matrix. corr Correlation matrix test statistics current analysis. dim = k * length(w). alpha Overall alpha. cum_alpha Cumulative alpha spent analysis. required type = 1. maxpts GenzBretz function maximum number function values integer. abseps GenzBretz function absolute error tolerance. tol Find root tolerance. sf list alpha spending functions spend alpha hypotheses. type = 0 3 length equals number hypotheses. type = 1 sf needed. type = 2 first component used. sfparm list parameters supplied sfs. type = 0 3 length equals number hypotheses. type = 1 sfparm needed. type = 2 first component used. t list information fraction used alpha spending, may different actual information fraction. component corresponds hypothesis. type = 0 3 length equals number hypotheses. type = 1 t needed. type = 2 first component used. ... Additional arguments.","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_bounds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute p-value boundaries of the parametric MTP method with overall alpha spending for all hypotheses — generate_bounds","text":"tibble k * (2^(n_hypotheses - 1)) rows p-value boundaries. Inflation factor also provided type = 3.","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_bounds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute p-value boundaries of the parametric MTP method with overall alpha spending for all hypotheses — generate_bounds","text":"","code":"# Build the transition matrix m <- matrix(c(   0, 0.5, 0.5,   0.5, 0, 0.5,   0.5, 0.5, 0 ), nrow = 3, byrow = TRUE)  # Initialize weights w <- c(1 / 3, 1 / 3, 1 / 3)  # Input information fraction IF_IA <- c(155 / 305, 160 / 320, 165 / 335)  # Input event count of intersection of paired hypotheses - Table 2 event <- tibble::tribble(   ~H1, ~H2, ~Analysis, ~Event,   1, 1, 1, 155,   2, 2, 1, 160,   3, 3, 1, 165,   1, 2, 1, 85,   1, 3, 1, 85,   2, 3, 1, 85,   1, 1, 2, 305,   2, 2, 2, 320,   3, 3, 2, 335,   1, 2, 2, 170,   1, 3, 2, 170,   2, 3, 2, 170 )  # Generate correlation from events gs_corr <- generate_corr(event)  # Generate bounds generate_bounds(   type = 3,   k = 2,   w = w,   m = m,   corr = gs_corr,   alpha = 0.025,   sf = list(gsDesign::sfLDOF, gsDesign::sfLDOF, gsDesign::sfLDOF),   sfparm = list(0, 0, 0),   t = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[3], 1)) ) #> # A tibble: 14 × 6 #>    Analysis Hypotheses        H1        H2        H3    xi #>       <int> <chr>          <dbl>     <dbl>     <dbl> <dbl> #>  1        1 H1          0.00167  NA        NA         1    #>  2        1 H1, H2      0.000471  0.000423 NA         1.03 #>  3        1 H1, H2, H3  0.000223  0.000198  0.000177  1.04 #>  4        1 H1, H3      0.000470 NA         0.000382  1.02 #>  5        1 H2         NA         0.00153  NA         1    #>  6        1 H2, H3     NA         0.000421  0.000381  1.02 #>  7        1 H3         NA        NA         0.00140   1    #>  8        2 H1          0.0245   NA        NA         1    #>  9        2 H1, H2      0.0135    0.0135   NA         1.09 #> 10        2 H1, H2, H3  0.00949   0.00950   0.00951   1.15 #> 11        2 H1, H3      0.0135   NA         0.0135    1.09 #> 12        2 H2         NA         0.0245   NA         1    #> 13        2 H2, H3     NA         0.0134    0.0134    1.09 #> 14        2 H3         NA        NA         0.0245    1"},{"path":"https://merck.github.io/wpgsd/reference/generate_corr.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate correlation matrix based on event counts — generate_corr","title":"Generate correlation matrix based on event counts — generate_corr","text":"Generate correlation matrix based event counts","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_corr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate correlation matrix based on event counts — generate_corr","text":"","code":"generate_corr(event)"},{"path":"https://merck.github.io/wpgsd/reference/generate_corr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate correlation matrix based on event counts — generate_corr","text":"event Event count hypothesis analysis, including event count intersection hypotheses. contains 4 columns: H1, H2, Analysis, Event. H1 needs listed 1, 2, 3, etc. numbers.","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_corr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate correlation matrix based on event counts — generate_corr","text":"correlation matrix.","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_corr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate correlation matrix based on event counts — generate_corr","text":"","code":"# Build the transition matrix m <- matrix(c(   0, 0.5, 0.5,   0.5, 0, 0.5,   0.5, 0.5, 0 ), nrow = 3, byrow = TRUE) # initialize weights w <- c(1 / 3, 1 / 3, 1 / 3)  # Input event count of intersection of paired hypotheses - Table 2 event <- tibble::tribble(   ~H1, ~H2, ~Analysis, ~Event,   1, 1, 1, 155,   2, 2, 1, 160,   3, 3, 1, 165,   1, 2, 1, 85,   1, 3, 1, 85,   2, 3, 1, 85,   1, 1, 2, 305,   2, 2, 2, 320,   3, 3, 2, 335,   1, 2, 2, 170,   1, 3, 2, 170,   2, 3, 2, 170 )  # Generate correlation from events gs_corr <- generate_corr(event)"},{"path":"https://merck.github.io/wpgsd/reference/generate_event_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate table of event counts from ADSL and ADTTE datasets — generate_event_table","title":"Generate table of event counts from ADSL and ADTTE datasets — generate_event_table","text":"Generate table event counts ADSL ADTTE datasets","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_event_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate table of event counts from ADSL and ADTTE datasets — generate_event_table","text":"","code":"generate_event_table(paths, h_select, adsl_name, adtte_name, key_var, cnsr_var)"},{"path":"https://merck.github.io/wpgsd/reference/generate_event_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate table of event counts from ADSL and ADTTE datasets — generate_event_table","text":"paths vector paths analysis datasets. Length equal number analyses completed. h_select Selection criterion hypothesis. tibble containing 2 columns: Hypothesis Crit. adsl_name SAS dataset name subject-level analysis data. Usually \"adsl\". adtte_name SAS dataset name time--event analysis data. Usually \"adtte\". key_var Key variable join adsl adtte datasets. example, \"USUBJID\" \"SUBJID\". cnsr_var Variable indicate censoring (1 = censor; 0 = event). example, \"CNSR\".","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_event_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate table of event counts from ADSL and ADTTE datasets — generate_event_table","text":"list two components: event: event count table input generate_bounds(). dsets: analysis datasets hypothesis.","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_event_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate table of event counts from ADSL and ADTTE datasets — generate_event_table","text":"","code":"library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union  paths <- system.file(\"extdata/\", package = \"wpgsd\")  # Selection criteria for each hypothesis h_select <- tibble::tribble(   ~Hypothesis, ~Crit,   1, \"PARAMCD == 'OS' & TRT01P %in% c('Xanomeline High Dose', 'Placebo')\",   2, \"PARAMCD == 'OS' & TRT01P %in% c('Xanomeline Low Dose', 'Placebo')\" )  event <- generate_event_table(paths, h_select,   adsl_name = \"adsl\", adtte_name = \"adtte\",   key_var = \"USUBJID\", cnsr_var = \"CNSR\" )$event  event %>%   gt::gt() %>%   gt::tab_header(title = \"Event Count - Computed from SAS Datasets Example\")     Event Count - Computed from SAS Datasets Example     H1       H2       Analysis       Event     1 1 1 662 2 1 591 2 1 45"},{"path":"https://merck.github.io/wpgsd/reference/tidyeval.html","id":null,"dir":"Reference","previous_headings":"","what":"Tidy eval helpers — tidyeval","title":"Tidy eval helpers — tidyeval","text":"page lists tidy eval tools reexported package rlang. learn using tidy eval scripts packages high level, see dplyr programming vignette ggplot2 packages vignette. Metaprogramming section Advanced R may also useful deeper dive. tidy eval operators {{, !!, !!! syntactic constructs specially interpreted tidy eval functions. mostly need {{, !! !!! advanced operators use simple cases. curly-curly operator {{ allows tunnel data-variables passed function arguments inside tidy eval functions. {{ designed individual arguments. pass multiple arguments contained dots, use ... normal way.   enquo() enquos() delay execution one several function arguments. former returns single expression, latter returns list expressions. defused, expressions longer evaluate . must injected back evaluation context !! (single expression) !!! (list expressions).   simple case, code equivalent usage {{ ... . Defusing enquo() enquos() needed complex cases, instance need inspect modify expressions way. .data pronoun object represents current slice data. variable name string, use .data pronoun subset variable [[.   Another tidy eval operator :=. makes possible use glue curly-curly syntax LHS =. technical reasons, R language support complex expressions left =, use := workaround.   Many tidy eval functions like dplyr::mutate() dplyr::summarise() give automatic name unnamed inputs. need create sort automatic names , use as_label(). instance, glue-tunnelling syntax can reproduced manually :   Expressions defused enquo() (tunnelled {{) need simple column names, can arbitrarily complex. as_label() handles cases gracefully. code assumes simple column name, use as_name() instead. safer throws error input name expected.","code":"my_function <- function(data, var, ...) {   data %>%     group_by(...) %>%     summarise(mean = mean({{ var }})) } my_function <- function(data, var, ...) {   # Defuse   var <- enquo(var)   dots <- enquos(...)    # Inject   data %>%     group_by(!!!dots) %>%     summarise(mean = mean(!!var)) } my_var <- \"disp\" mtcars %>% summarise(mean = mean(.data[[my_var]])) my_function <- function(data, var, suffix = \"foo\") {   # Use `{{` to tunnel function arguments and the usual glue   # operator `{` to interpolate plain strings.   data %>%     summarise(\"{{ var }}_mean_{suffix}\" := mean({{ var }})) } my_function <- function(data, var, suffix = \"foo\") {   var <- enquo(var)   prefix <- as_label(var)   data %>%     summarise(\"{prefix}_mean_{suffix}\" := mean(!!var)) }"},{"path":"https://merck.github.io/wpgsd/reference/wpgsd-package.html","id":null,"dir":"Reference","previous_headings":"","what":"wpgsd: Weighted Parametric Group Sequential Design — wpgsd-package","title":"wpgsd: Weighted Parametric Group Sequential Design — wpgsd-package","text":"Adjusted inference weighted parametric group sequential design. Weighted parametric group sequential design (WPGSD) Anderson et al. (2022) doi:10.1002/bimj.202100085  allows one take advantage known correlation structure constructing efficacy bounds control family-wise error rate (FWER) group sequential design. , correlation may due common observations nested populations, due common observations overlapping populations, due common observations control arm.","code":""},{"path":[]},{"path":"https://merck.github.io/wpgsd/reference/wpgsd-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"wpgsd: Weighted Parametric Group Sequential Design — wpgsd-package","text":"Maintainer: Yujie Zhao yujie.zhao@merck.com Authors: Keaven Anderson keaven_anderson@merck.com Zifang Guo zifang.guo@merck.com Jing Zhao jing_zhaox@merck.com Linda Sun linda_sun@merck.com Yi Cui yi.cui@merck.com Larry Leon larry.leon2@merck.com contributors: Merck Sharp & Dohme Corp [copyright holder]","code":""},{"path":"https://merck.github.io/wpgsd/news/index.html","id":"wpgsd-010","dir":"Changelog","previous_headings":"","what":"wpgsd 0.1.0","title":"wpgsd 0.1.0","text":"Initial release. wpgsd package now available GitHub, install prefer use specific version, install v0.1.0 GitHub release version number.","code":"remotes::install_github(\"Merck/wpgsd\") remotes::install_github(\"Merck/wpgsd@v0.1.0\")"}]
