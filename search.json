[{"path":"https://merck.github.io/wpgsd/articles/adj-seq-p.html","id":"example-overview","dir":"Articles","previous_headings":"","what":"Example overview","title":"Adjusted sequential p-values","text":"2-arm controlled clinical trial example one primary endpoint, 3 patient populations defined status two biomarkers B: biomarker positive, biomarker B positive, overall population. 3 primary elementary hypotheses : \\(H_1\\): experimental treatment superior control biomarker positive population; \\(H_2\\): experimental treatment superior control biomarker B positive population; \\(H_3\\): experimental treatment superior control overall population. Assume interim analysis final analysis planned study number events listed observed p-values information fraction \\(H_1, H_2, H_3\\) IA assign initial weights \\(H_1, H_2, H_3\\) \\(\\left(w_1(), w_2(), w_3() \\right) = (0.3, 0.3, 0.4).\\) multiplicity strategy visualized . \\(H_1\\) rejected, \\(3/7\\) local significance level \\(\\alpha_1\\) propagated \\(H_2\\), \\(4/7\\) go \\(H_3\\). \\(H_3\\) rejected, half \\(\\alpha_3\\) goes \\(H_1\\), half goes \\(H_2\\).  correlation 6 statistisc (2 analysis \\(\\times\\) 3 hypothesis) ","code":"event_tbl <- tribble(   ~population, ~analysis, ~event,   \"A positive\",     1,       80,   \"B positive\",     1,       88,   \"AB positive\",    1,       64,   \"overall\",        1,       180,   \"A positive\",     2,       160,   \"B positive\",     2,       176,   \"AB positive\",    2,       128,   \"overall\",        2,       360, ) obs_tbl <- tribble(   ~hypothesis, ~analysis, ~obs_p,     \"H1\",          1,       0.02,     \"H2\",          1,       0.01,     \"H3\",          1,       0.006,     \"H1\",          2,       0.015,     \"H2\",          2,       0.012,     \"H3\",          2,       0.004) %>%   mutate(obs_Z = -qnorm(obs_p))  obs_tbl %>%   gt() %>%   tab_header(title = \"Nominal p-values\") p_obs_IA <- (obs_tbl %>% filter(analysis == 1))$obs_p p_obs_FA <- (obs_tbl %>% filter(analysis == 2))$obs_p IF_IA <- c(   ((event_tbl %>% filter(analysis == 1, population == \"A positive\"))$event + (event_tbl %>% filter(analysis == 1, population == \"overall\"))$event)/     ((event_tbl %>% filter(analysis == 2, population == \"A positive\"))$event + (event_tbl %>% filter(analysis == 2, population == \"overall\"))$event),   ((event_tbl %>% filter(analysis == 1, population == \"B positive\"))$event + (event_tbl %>% filter(analysis == 1, population == \"overall\"))$event)/     ((event_tbl %>% filter(analysis == 2, population == \"B positive\"))$event + (event_tbl %>% filter(analysis == 2, population == \"overall\"))$event),   ((event_tbl %>% filter(analysis == 1, population == \"AB positive\"))$event + (event_tbl %>% filter(analysis == 1, population == \"overall\"))$event)/     ((event_tbl %>% filter(analysis == 2, population == \"AB positive\"))$event + (event_tbl %>% filter(analysis == 2, population == \"overall\"))$event) )  IF_IA ## [1] 0.5 0.5 0.5 # Transition matrix in Figure A1 m <- matrix(c(     0, 3/7, 4/7,   3/7,   0, 4/7,   1/2, 1/2,   0 ), nrow = 3, byrow = TRUE) # Initial weights w <- c(0.3, 0.3, 0.4) name_hypotheses <- c(\"H1: Biomarker A positive\", \"H2: Biomarker B positive\",  \"H3: Overall Population\")  hplot <- gMCPLite::hGraph(   3,alphaHypotheses=w,m=m,   nameHypotheses=name_hypotheses, trhw=.2, trhh=.1,   digits=5, trdigits=3, size=5, halfWid=1, halfHgt=0.5,   offset=0.2 , trprop= 0.4,   fill=as.factor(c(2,3,1)),   palette = c(\"#BDBDBD\", \"#E0E0E0\", \"#EEEEEE\"),   wchar = \"w\") hplot # Event count of intersection of paired hypotheses - Table 2 # H1, H2: Hypotheses intersected. # (1, 1) represents counts for hypothesis 1 # (1, 2) for counts for the intersection of hypotheses 1 and 2 event <- tribble(   ~H1, ~H2, ~Analysis, ~Event,   1,    1,      1,     event_tbl %>% filter(analysis == 1, population == \"A positive\") %>% select(event) %>% as.numeric(),   2,    2,      1,     event_tbl %>% filter(analysis == 1, population == \"B positive\") %>% select(event) %>% as.numeric(),   3,    3,      1,     event_tbl %>% filter(analysis == 1, population == \"overall\") %>% select(event) %>% as.numeric(),   1,    2,      1,     event_tbl %>% filter(analysis == 1, population == \"AB positive\") %>% select(event) %>% as.numeric(),   1,    3,      1,     event_tbl %>% filter(analysis == 1, population == \"A positive\") %>% select(event) %>% as.numeric(),   2,    3,      1,     event_tbl %>% filter(analysis == 1, population == \"B positive\") %>% select(event) %>% as.numeric(),   1,    1,      2,     event_tbl %>% filter(analysis == 2, population == \"A positive\") %>% select(event) %>% as.numeric(),   2,    2,      2,     event_tbl %>% filter(analysis == 2, population == \"B positive\") %>% select(event) %>% as.numeric(),   3,    3,      2,     event_tbl %>% filter(analysis == 2, population == \"overall\") %>% select(event) %>% as.numeric(),   1,    2,      2,     event_tbl %>% filter(analysis == 2, population == \"AB positive\") %>% select(event) %>% as.numeric(),   1,    3,      2,     event_tbl %>% filter(analysis == 2, population == \"A positive\") %>% select(event) %>% as.numeric(),   2,    3,      2,     event_tbl %>% filter(analysis == 2, population == \"B positive\") %>% select(event) %>% as.numeric()) event ## # A tibble: 12 × 4 ##       H1    H2 Analysis Event ##    <dbl> <dbl>    <dbl> <dbl> ##  1     1     1        1    80 ##  2     2     2        1    88 ##  3     3     3        1   180 ##  4     1     2        1    64 ##  5     1     3        1    80 ##  6     2     3        1    88 ##  7     1     1        2   160 ##  8     2     2        2   176 ##  9     3     3        2   360 ## 10     1     2        2   128 ## 11     1     3        2   160 ## 12     2     3        2   176 # Generate correlation from events gs_corr <- wpgsd::generate_corr(event) gs_corr %>% round(2) ##      H1_A1 H2_A1 H3_A1 H1_A2 H2_A2 H3_A2 ## [1,]  1.00  0.76  0.67  0.71  0.54  0.47 ## [2,]  0.76  1.00  0.70  0.54  0.71  0.49 ## [3,]  0.67  0.70  1.00  0.47  0.49  0.71 ## [4,]  0.71  0.54  0.47  1.00  0.76  0.67 ## [5,]  0.54  0.71  0.49  0.76  1.00  0.70 ## [6,]  0.47  0.49  0.71  0.67  0.70  1.00"},{"path":[]},{"path":"https://merck.github.io/wpgsd/articles/adj-seq-p.html","id":"ia","dir":"Articles","previous_headings":"Sequantial p-value","what":"IA","title":"Adjusted sequential p-values","text":"","code":"seq_p_IA_H123 <- calc_seq_p(   test_analysis = 1,            # stage of interest   test_hypothesis = \"H1, H2, H3\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.15)   )  seq_p_IA_H12 <- calc_seq_p(   test_analysis = 1,            # stage of interest   test_hypothesis = \"H1, H2\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.15)   )  seq_p_IA_H13 <- calc_seq_p(   test_analysis = 1,            # stage of interest   test_hypothesis = \"H1, H3\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.15)   )  seq_p_IA_H23 <- calc_seq_p(   test_analysis = 1,            # stage of interest   test_hypothesis = \"H2, H3\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.15)   )  seq_p_IA_H1 <- calc_seq_p(   test_analysis = 1,            # stage of interest   test_hypothesis = \"H1\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.2)   )  seq_p_IA_H2 <- calc_seq_p(   test_analysis = 1,            # stage of interest   test_hypothesis = \"H2\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.2)   )  seq_p_IA_H3 <- calc_seq_p(   test_analysis = 1,            # stage of interest   test_hypothesis = \"H3\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.2)   ) seq_p_IA_H123_B <- calc_seq_p(   test_analysis = 1,            # stage of interest   test_hypothesis = \"H1, H2, H3\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2)   )  seq_p_IA_H12_B <- calc_seq_p(   test_analysis = 1,            # stage of interest   test_hypothesis = \"H1, H2\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2)   )  seq_p_IA_H13_B <- calc_seq_p(   test_analysis = 1,            # stage of interest   test_hypothesis = \"H1, H3\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2)   )  seq_p_IA_H23_B <- calc_seq_p(   test_analysis = 1,            # stage of interest   test_hypothesis = \"H2, H3\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2)   )  seq_p_IA_H1_B <- calc_seq_p(   test_analysis = 1,            # stage of interest   test_hypothesis = \"H1\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2)   )  seq_p_IA_H2_B <- calc_seq_p(   test_analysis = 1,            # stage of interest   test_hypothesis = \"H2\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2)   )  seq_p_IA_H3_B <- calc_seq_p(   test_analysis = 1,            # stage of interest   test_hypothesis = \"H3\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2)   )"},{"path":"https://merck.github.io/wpgsd/articles/adj-seq-p.html","id":"fa","dir":"Articles","previous_headings":"Sequantial p-value","what":"FA","title":"Adjusted sequential p-values","text":"","code":"seq_p_FA_H123 <- calc_seq_p(   test_analysis = 2,            # stage of interest   test_hypothesis = \"H1, H2, H3\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.15)   )  seq_p_FA_H12 <- calc_seq_p(   test_analysis = 2,            # stage of interest   test_hypothesis = \"H1, H2\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.15)   )  seq_p_FA_H13 <- calc_seq_p(   test_analysis = 2,            # stage of interest   test_hypothesis = \"H1, H3\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.15)   )  seq_p_FA_H23 <- calc_seq_p(   test_analysis = 2,            # stage of interest   test_hypothesis = \"H2, H3\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.15)   )  seq_p_FA_H1 <- calc_seq_p(   test_analysis = 2,            # stage of interest   test_hypothesis = \"H1\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.2)   )  seq_p_FA_H2 <- calc_seq_p(   test_analysis = 2,            # stage of interest   test_hypothesis = \"H2\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.2)   )  seq_p_FA_H3 <- calc_seq_p(   test_analysis = 2,            # stage of interest   test_hypothesis = \"H3\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 2,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = gsDesign::sfHSD,   spending_fun_par = -4,   info_frac = c(min(IF_IA), 1),   interval = c(1e-4, 0.2)   ) seq_p_FA_H123_B <- calc_seq_p(   test_analysis = 2,            # stage of interest   test_hypothesis = \"H1, H2, H3\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2)   )  seq_p_FA_H12_B <- calc_seq_p(   test_analysis = 2,            # stage of interest   test_hypothesis = \"H1, H2\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2)   )  seq_p_FA_H13_B <- calc_seq_p(   test_analysis = 2,            # stage of interest   test_hypothesis = \"H1, H3\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2)   )  seq_p_FA_H23_B <- calc_seq_p(   test_analysis = 2,            # stage of interest   test_hypothesis = \"H2, H3\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2)   )  seq_p_FA_H1_B <- calc_seq_p(   test_analysis = 2,            # stage of interest   test_hypothesis = \"H1\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2)   )  seq_p_FA_H2_B <- calc_seq_p(   test_analysis = 2,            # stage of interest   test_hypothesis = \"H2\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2)   )  seq_p_FA_H3_B <- calc_seq_p(   test_analysis = 2,            # stage of interest   test_hypothesis = \"H3\",   p_obs = p_obs_IA,             # observed p-value   alpha_spending_type = 0,   n_analysis = 2,   initial_weight = w,   transition_mat = m,   z_corr = gs_corr,   spending_fun = list(gsDesign::sfHSD, gsDesign::sfHSD, gsDesign::sfHSD),   spending_fun_par = list(-4, -4, -4),   info_frac = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[2], 1)),   interval = c(1e-4, 0.2)   )"},{"path":[]},{"path":"https://merck.github.io/wpgsd/articles/adj-seq-p.html","id":"ia-1","dir":"Articles","previous_headings":"Adjusted-Sequential p-value","what":"IA","title":"Adjusted sequential p-values","text":"","code":"adj_seq_IA_H1 <- max(seq_p_IA_H123, seq_p_IA_H12, seq_p_IA_H13, seq_p_IA_H1) adj_seq_IA_H2 <- max(seq_p_IA_H123, seq_p_IA_H12, seq_p_IA_H23, seq_p_IA_H2) adj_seq_IA_H3 <- max(seq_p_IA_H123, seq_p_IA_H13, seq_p_IA_H23, seq_p_IA_H3)  cat(\"The adjusted-sequential p-value of H1, H2, H3 in IA via WPGSD is\", adj_seq_IA_H1, adj_seq_IA_H2, adj_seq_IA_H3, \"\\n\") ## The adjusted-sequential p-value of H1, H2, H3 in IA via WPGSD is 0.1677811 0.1399851 0.1008572 adj_seq_IA_H1_B <- max(seq_p_IA_H123_B, seq_p_IA_H12_B, seq_p_IA_H13_B, seq_p_IA_H1_B) adj_seq_IA_H2_B <- max(seq_p_IA_H123_B, seq_p_IA_H12_B, seq_p_IA_H23_B, seq_p_IA_H2_B) adj_seq_IA_H3_B <- max(seq_p_IA_H123_B, seq_p_IA_H13_B, seq_p_IA_H23_B, seq_p_IA_H3_B)  cat(\"The adjusted-sequential p-value of H1, H2, H3 in FA via weighted Bonferroni is\", adj_seq_IA_H1_B, adj_seq_IA_H2_B, adj_seq_IA_H3_B, \"\\n\") ## The adjusted-sequential p-value of H1, H2, H3 in FA via weighted Bonferroni is 0.1677811 0.1677811 0.1258358"},{"path":[]},{"path":"https://merck.github.io/wpgsd/articles/adj-seq-p.html","id":"wpgsd","dir":"Articles","previous_headings":"Adjusted-Sequential p-value > FA","what":"WPGSD","title":"Adjusted sequential p-values","text":"","code":"adj_seq_FA_H1 <- max(seq_p_FA_H123, seq_p_FA_H12, seq_p_FA_H13, seq_p_FA_H1) adj_seq_FA_H2 <- max(seq_p_FA_H123, seq_p_FA_H12, seq_p_FA_H23, seq_p_FA_H2) adj_seq_FA_H3 <- max(seq_p_FA_H123, seq_p_FA_H13, seq_p_FA_H23, seq_p_FA_H3)  cat(\"The adjusted-sequential p-value of H1, H2, H3 in FA via WPGSD is\", adj_seq_FA_H1, adj_seq_FA_H2, adj_seq_FA_H3, \"\\n\") ## The adjusted-sequential p-value of H1, H2, H3 in FA via WPGSD is 0.02107688 0.01765037 0.01283024 adj_seq_FA_H1_B <- max(seq_p_FA_H123_B, seq_p_FA_H12_B, seq_p_FA_H13_B, seq_p_FA_H1_B) adj_seq_FA_H2_B <- max(seq_p_FA_H123_B, seq_p_FA_H12_B, seq_p_FA_H23_B, seq_p_FA_H2_B) adj_seq_FA_H3_B <- max(seq_p_FA_H123_B, seq_p_FA_H13_B, seq_p_FA_H23_B, seq_p_FA_H3_B)  cat(\"The adjusted-sequential p-value of H1, H2, H3 in FA via weighted Bonferroni is\", adj_seq_FA_H1_B, adj_seq_FA_H2_B, adj_seq_FA_H3_B, \"\\n\") ## The adjusted-sequential p-value of H1, H2, H3 in FA via weighted Bonferroni is 0.02126682 0.02126682 0.01604666"},{"path":"https://merck.github.io/wpgsd/articles/adj-seq-p.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Adjusted sequential p-values","text":"","code":"ans <- tribble(   ~Analysis, ~Hypothesis, ~`Sequantial p-values of WPGSD`,  ~`Sequantial p-values of Weighted Bonferroni`, ~`Adjusted-sequantial p-values of WPGSD`, ~`Adjusted-sequantial p-values of Weighted Bonferroni`,   \"IA\",       \"H123\",       seq_p_IA_H123,      seq_p_IA_H123_B,    NA,                    NA,   \"IA\",       \"H12\",        seq_p_IA_H12,       seq_p_IA_H12_B,     NA,                    NA,   \"IA\",       \"H13\",        seq_p_IA_H13,       seq_p_IA_H13_B,     NA,                    NA,   \"IA\",       \"H23\",        seq_p_IA_H23,       seq_p_IA_H23_B,     NA,                    NA,   \"IA\",       \"H1\",         seq_p_IA_H1,        seq_p_IA_H1_B,      adj_seq_IA_H1,         adj_seq_IA_H1_B,   \"IA\",       \"H2\",         seq_p_IA_H2,        seq_p_IA_H2_B,      adj_seq_IA_H2,         adj_seq_IA_H2_B,   \"IA\",       \"H3\",         seq_p_IA_H3,        seq_p_IA_H3_B,      adj_seq_IA_H3,         adj_seq_IA_H3_B,   \"FA\",       \"H123\",       seq_p_FA_H123,      seq_p_FA_H123_B,    NA,                    NA,   \"FA\",       \"H12\",        seq_p_FA_H12,       seq_p_FA_H12_B,     NA,                    NA,   \"FA\",       \"H13\",        seq_p_FA_H13,       seq_p_FA_H13_B,     NA,                    NA,   \"FA\",       \"H23\",        seq_p_FA_H23,       seq_p_FA_H23_B,     NA,                    NA,   \"FA\",       \"H1\",         seq_p_FA_H1,        seq_p_FA_H1_B,      adj_seq_FA_H1,         adj_seq_FA_H1_B,   \"FA\",       \"H2\",         seq_p_FA_H2,        seq_p_FA_H2_B,      adj_seq_FA_H2,         adj_seq_FA_H2_B,   \"FA\",       \"H3\",         seq_p_FA_H3,        seq_p_FA_H3_B,      adj_seq_FA_H3,         adj_seq_FA_H3_B   )  ans %>%   gt::gt() %>%   gt::tab_style_body(     columns = where(is.numeric),     style = cell_fill(color = \"pink\"),     fn = function(x) x <= 0.025   ) %>%   gt::fmt_number(columns = 3:6, decimals = 4) %>%   gt::tab_header(title = \"(Adjusted-) sequantial p-values\",                  subtitle = \"Multiple experiment arm vs. common control\") # %>% gt::as_latex()"},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"background","dir":"Articles","previous_headings":"","what":"Background","title":"Quickstart guide","text":"weighted parametric group sequential design (WPGSD) (Anderson et al. (2022)) approach allows one take advantage known correlation structure constructing efficacy bounds control family-wise error rate (FWER) group sequential design. correlation may due common observations nested populations, due common observations overlapping populations, due common observations control arm. document illustrates use R package wpgsd implement approach.","code":""},{"path":[]},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"closed-testing-and-parametric-tests","dir":"Articles","previous_headings":"Methods and Examples","what":"Closed Testing and Parametric Tests","title":"Quickstart guide","text":"aim control familywise error rate (FWER) level \\(\\alpha\\). Let \\(J \\subseteq \\). intersection hypothesis \\(H_J\\) assumes null hypothesis individual hypotheses \\(H_i\\) \\(\\J\\). Closed testing principle follows: sets \\(J \\subseteq \\) \\(j \\J\\), \\(H_J\\) can rejected level \\(\\alpha\\), \\(H_j\\) can rejected. Weighted parametric tests can used : Bretz et al. (2011), Xi et al. (2017) fixed designs Maurer Bretz (2013) group sequential.","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"consonance","dir":"Articles","previous_headings":"Methods and Examples","what":"Consonance","title":"Quickstart guide","text":"closed procedure called consonant rejection complete intersection null hypothesis \\(H_I\\) implies least one elementary hypothesis \\(H_i, \\\\), rejected. Consonance desirable property leading short-cut procedures give rejection decisions original closed procedure fewer operations. WPGSD, consonance always hold general closed-testing procedure required.","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"group-sequential-design-notations-and-assumptions","dir":"Articles","previous_headings":"Methods and Examples","what":"Group Sequential Design Notations and Assumptions","title":"Quickstart guide","text":"set \\(\\) hypotheses \\(\\\\). \\(K\\) group sequential analyses, \\(k = 1, \\ldots, K\\) required, can generalized Assume tests \\(Z_{ik}\\), \\(\\\\), \\(1 \\leq k \\leq K\\) large \\(Z_{ik}\\) used reject \\(H_i\\)","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"correlation-structure","dir":"Articles","previous_headings":"Methods and Examples","what":"Correlation Structure","title":"Quickstart guide","text":"Events individual hypothesis \\(H_i\\),\\(\\\\) analysis k denoted \\(n_{ik}\\). Assume endpoint hypotheses (can relaxed) binary continuous outcomes \\(n_{ik}\\) represents sample size \\(Z_{ik}\\) standardized normal test treatment effect individual hypothesis \\(H_i\\) analysis \\(k\\) Denote \\(n_{\\wedge ^\\prime,k\\wedge k^\\prime}\\) number observations (events) included \\(Z_{ik}\\) \\(Z_{^\\prime k^\\prime}\\), \\(\\\\), \\(1\\le k\\le K\\). Key result \\[  \\hbox{Corr}(Z_{ik}, Z_{^\\prime k^\\prime }) =  \\frac{n_{\\wedge ^\\prime ,k\\wedge k^\\prime }}{\\sqrt{n_{ik}n_{^\\prime k^\\prime }}} \\] Proof builds standard group sequential theory (Chen et al. (2021))","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"example-1-overlapping-populations-two-analyses","dir":"Articles","previous_headings":"Methods and Examples","what":"Example 1: Overlapping Populations, Two Analyses","title":"Quickstart guide","text":"Following illustrates first example, overlapping populations (e.g. due biomarker) also example 1 Anderson et al. (2022). Ex1: Populations multiplicity strategy defined follows.  event count hypothesis analysis shown . Number events analysis population Example 1. IA: interim analysis. FA: final analysis. correlation matrix among test statistics follows. Correlation Matrix Test Statistics Example 1. Identical numeric values (lower triangular) formulas (upper triangular) shown.","code":"# Transition matrix m <- matrix(c(   0, 0, 1,   0, 0, 1,   0.5, 0.5, 0 ), nrow = 3, byrow = TRUE) # Weight matrix w <- c(0.3, 0.3, 0.4)  # Multiplicity graph cbPalette <- c(\"#999999\", \"#E69F00\", \"#56B4E9\")  nameHypotheses <- c(   \"H1: Population 1\",   \"H2: Population 2\",   \"H3: Overall Population\" )  hplot <- hGraph(3,   alphaHypotheses = w,   m = m,   nameHypotheses = nameHypotheses,   trhw = .2, trhh = .1,   digits = 5, trdigits = 3, size = 5, halfWid = 1,   halfHgt = 0.5, offset = 0.2, trprop = 0.4,   fill = as.factor(c(2, 3, 1)),   palette = cbPalette,   wchar = \"w\" ) hplot"},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"example-2-common-control-two-analyses","dir":"Articles","previous_headings":"Methods and Examples","what":"Example 2: Common Control, Two Analyses","title":"Quickstart guide","text":"Following illustrates second example correlation comes common control arm. also example 2 Anderson et al. (2022).  Number events analysis treatment arm Example 2. IA: interim analysis. FA: final analysis. Correlation Matrix Example 2. Identical numeric values (lower triangular) formulas (upper triangular) shown.","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"hypotheses-set","dir":"Articles","previous_headings":"Methods and Examples","what":"Hypotheses Set","title":"Quickstart guide","text":"2 examples 7 intersection hypotheses corresponding weighting strategies illustrated . Weighting strategy Example 1. Weighting strategy Example 2.","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"alpha-spending-3-approaches","dir":"Articles","previous_headings":"Methods and Examples","what":"\\(\\alpha\\) Spending: 3 approaches","title":"Quickstart guide","text":"WPGSD approach uses known correlations tests study. relaxes bounds allows increased power smaller sample size. Three spending approaches proposed: Fixed spending (Fleming-Harrington-O’Brien (FHO) approach). Specify \\(0 < \\alpha_1(J) < \\alpha_2(J) < \\ldots < \\alpha_K(J) = \\alpha(J) \\leq \\alpha\\) \\(J\\subseteq \\), \\(\\alpha(J)\\) total alpha intersection hypothesis \\(H_J\\) according graphical approach. \\(\\alpha\\)-spending approach 1. choose spending function family \\(f(t,\\alpha)\\) set \\(\\alpha_k(J)=f(t_k(J),\\alpha(J))\\) \\(1\\le k\\le K\\) intersection hypotheses \\(J\\subseteq \\). \\(\\alpha\\)-spending approach 2. elementary hypothesis \\(\\) (\\(\\) = 1, 2, , \\(m\\)), specify \\(\\alpha\\)-spending function family \\(f_i(t,\\gamma)\\) \\(\\gamma\\) \\(\\alpha\\) level hypothesis \\(f_i(t_{ik},\\gamma)\\) determines much \\(\\alpha\\) spend analysis \\(k\\) hypothesis \\(\\) level \\(\\gamma\\) allocated hypothesis. \\(\\alpha_k(J) = \\sum_{\\J} f_i(t_{ik}, w_i(J)\\alpha)\\).","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"bounds-computation-parametric-test-fixed-design-for-example-two-populations-one-analysis","dir":"Articles","previous_headings":"Methods and Examples","what":"Bounds Computation: Parametric Test, Fixed Design (For Example, Two Populations, One Analysis)","title":"Quickstart guide","text":"Assume (\\(Z_1,Z_2\\)) bivariate normal known correlation Find \\(\\alpha\\)-inflation factor \\(c_J\\) \\[ \\alpha = P[\\cup_{\\J} \\{p_i \\leq c_Jw_{J,}\\alpha \\}] = P[\\cup_{\\J} \\{Z_i \\geq \\Phi^{-1}(1-c_Jw_{J,}\\alpha \\}]\\] Basic algorithm code Bretz et al. (2011)","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"bounds-computation-wpgsd---fixed-spending-and-alpha-spending-approach-1","dir":"Articles","previous_headings":"Methods and Examples > Bounds Computation: Parametric Test, Fixed Design (For Example, Two Populations, One Analysis)","what":"Bounds Computation: WPGSD - Fixed spending and \\(\\alpha\\) spending approach 1","title":"Quickstart guide","text":"Assume \\(j < k\\) bounds \\(c_{ij} (J), \\J, j < k\\), already set remain unchanged. analysis \\(k\\), compute correlation matrix \\(Z_{ij}\\), \\(\\J\\), \\(j = 1, \\ldots, k\\). Initialize \\(\\alpha_{k}^{*}(J) = \\alpha_{k}(J) - \\alpha_{k-1}(J)\\). ii Set \\(b_{ik} = \\Phi^{-1}(1 - w_{}(J)\\alpha_{k}^{*} (J))\\), \\(\\J\\). iii Compute type error rate analysis \\(k\\) \\[ 1 - Pr(\\cap_{\\J} \\{ Z_{ik} < b_{ik} \\} \\cap_{\\J, j < k} \\{ Z_{ij} < c_{ij}(J) \\} ). \\] iv Update \\(\\alpha_{k}^{*}(J)\\) using root-finding steps ii - iii type error rate analysis \\(k\\) controlled \\(\\alpha_{k}(J)\\) \\(H_J\\). , \\[ 1 - Pr(\\cap_{\\J} \\{ Z_{ik} < b_{ik} \\} \\cap_{\\J, j < k} \\{ Z_{ij} < c_{ij}(J) \\} ) = \\alpha_{k}. \\] v Set \\(c_{ik}(J) = b_{ik}\\) previous step. corresponding nominal \\(p\\)-value boundary \\(p_{ik}(J)= 1-\\Phi(c_{ik}(J)) = w_i(J)\\alpha_k^*(J)\\). Note: interim bound depend future analyses. Solution requires root finding single \\(\\alpha_{k}^{*}(J)\\) time, \\(k = 1, \\ldots, K\\). Requires multivariate normal computation mvtnorm R package Genz et al. (2020).","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"bounds-computation-wpgsd---alpha-spending-approach-2","dir":"Articles","previous_headings":"Methods and Examples > Bounds Computation: Parametric Test, Fixed Design (For Example, Two Populations, One Analysis)","what":"Bounds Computation: WPGSD - \\(\\alpha\\) spending approach 2","title":"Quickstart guide","text":"Assume \\(j < k\\) bounds \\(c_{ij} (J), \\J, j < k\\), already set remain unchanged. analysis \\(k\\), compute correlation matrix \\(Z_{ij}\\), \\(\\J\\), \\(j = 1, \\ldots, k\\). Determine nominal \\(p\\)-value boundary elementary hypothesis \\(J\\) weighted Bonferroni test group sequential design described Maurer Bretz (2013). Let nominal \\(p\\)-value boundaries \\(\\alpha^\\prime_{ik}(J)\\). ii Choose inflation factor \\(\\xi_{k}(J) > 1\\) set \\[b_{ik} = \\Phi^{-1}(1 - \\xi_k(J) \\alpha^\\prime_{ik}(J)).\\] iii Update \\(\\xi_k(J)\\) type error rate analysis \\(k\\) controlled \\(\\alpha_{k}(J)\\) \\(H_J\\). , \\[ 1 - Pr(\\cap_{\\J} \\{ Z_{ik} < b_{ik} \\} \\cap_{\\J, j < k} \\{ Z_{ij} < c_{ij}(J) \\} ) = \\alpha_{k}(J).\\] iv appropriate \\(\\xi_k(J)\\) derived, nominal \\(p\\)-value boundaries \\(p_{ik}(J)=\\xi_k(J) \\alpha^\\prime_{ik}(J)\\), \\(b_{ik}\\) computed step ii, set \\(c_{ik}(J) = b_{ik}\\). Note: interim bound depend future analyses. Solution requires root finding single \\(\\xi_k(J)\\) time, \\(k = 1, \\ldots, K\\). Requires multivariate normal computation mvtnorm R package Genz et al. (2020).","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"implementation-of-example-1-with-overlapping-populations","dir":"Articles","previous_headings":"Methods and Examples","what":"Implementation of Example 1 with Overlapping Populations","title":"Quickstart guide","text":"first define transition matrix weights shown Section 2.5. Next set event count table follows: Analysis: Analysis number (1 interim, 2 final). Event: Event counts. (1, 1) represents counts hypothesis 1 (1, 2) counts intersection hypotheses 1 2 compute correlation matrix using event count table generate_corr(). see correlations accounted Bonferroni approach substantial , thus, might expect non-trivial impact bounds hypothesis tests. Bonferroni WPGSD bounds can computed via generate_bounds(). example, useHSD(-4) \\(\\alpha\\)-spending hypotheses. note, generate_bounds() input type specifies boundary type. 0 = Bonferroni. Separate alpha spending hypotheses. 1 = Fixed alpha spending hypotheses. Method 3a manuscript. 2 = Overall alpha spending hypotheses. Method 3b manuscript. 3 = Separate alpha spending hypotheses. Method 3c manuscript. Compute Bonferroni bounds. Compute WPGSD Bounds using \\(\\alpha\\)-spending approach 1 HSD(-4) spending. spending time defined minimum 3 observed information fractions. shows comparison Bonferroni WPGSD bounds. Nominal level final analysis using WPGSD method increased 1.3× obtained via Bonferroni approach. Closed testing procedure can performed using closed_test().","code":"event <- tribble(   ~H1, ~H2, ~Analysis, ~Event,   1, 1, 1, 100,   2, 2, 1, 110,   3, 3, 1, 225,   1, 2, 1, 80,   1, 3, 1, 100,   2, 3, 1, 110,   1, 1, 2, 200,   2, 2, 2, 220,   3, 3, 2, 450,   1, 2, 2, 160,   1, 3, 2, 200,   2, 3, 2, 220 ) event %>%   gt() %>%   tab_header(title = \"Event Count\") # Alternatively, one can manually enter paths for analysis datasets, # example below uses an example dataset assuming currently we are at IA1. paths <- system.file(\"extdata/\", package = \"wpgsd\")  ### Generate event count table from ADSL and ADTTE datasets # Selection criteria for each hypothesis h_select <- tribble(   ~Hypothesis, ~Crit,   1, \"PARAMCD=='OS' & TRT01P %in% c('Xanomeline High Dose', 'Placebo')\",   2, \"PARAMCD=='OS' & TRT01P %in% c('Xanomeline Low Dose', 'Placebo')\" )  event2 <- generate_event_table(paths, h_select,   adsl_name = \"adsl\", adtte_name = \"adtte\",   key_var = \"USUBJID\", cnsr_var = \"CNSR\" )$event  event2 %>%   gt() %>%   tab_header(title = \"Event Count - Computed from SAS Datasets Example\") ## Generate correlation from events corr <- generate_corr(event)  corr %>%   as_tibble() %>%   gt() %>%   fmt_number(columns = everything(), decimals = 2) %>%   tab_header(title = \"Correlation Matrix\") # Bonferroni bounds bound_Bonf <- generate_bounds(   type = 0, k = 2, w = w, m = m,   corr = corr, alpha = 0.025,   sf = list(sfHSD, sfHSD, sfHSD),   sfparm = list(-4, -4, -4),   t = list(c(0.5, 1), c(0.5, 1), c(0.5, 1)) )  bound_Bonf %>%   gt() %>%   fmt_number(columns = 3:5, decimals = 4) %>%   tab_header(title = \"Bonferroni bounds\") set.seed(1234) # WPGSD bounds, spending approach 1 bound_WPGSD <- generate_bounds(   type = 2, k = 2, w = w, m = m,   corr = corr, alpha = 0.025,   sf = sfHSD,   sfparm = -4,   t = c(min(100 / 200, 110 / 220, 225 / 450), 1) )  bound_WPGSD %>%   gt() %>%   fmt_number(columns = 3:5, decimals = 4) %>%   tab_header(title = \"WPGSD bounds\") ## Observed p-values. ## The tibble must contain columns Analysis, H1, H2 etc for all hypotheses p_obs <- tribble(   ~Analysis, ~H1, ~H2, ~H3,   1, 0.01, 0.0004, 0.03,   2, 0.05, 0.002, 0.015 )  ## Closed testing ## test_result <- closed_test(bound_WPGSD, p_obs)  p_obs %>%   gt() %>%   fmt_number(columns = 2:4, decimals = 8, drop_trailing_zeros = TRUE) %>%   tab_header(\"Observed Nominal p-Values\") test_result %>%   gt() %>%   tab_header(title = \"Closed Testing Results\")"},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"implementation-of-example-2-with-common-control","dir":"Articles","previous_headings":"Methods and Examples","what":"Implementation of Example 2 with Common Control","title":"Quickstart guide","text":"Similarly, codes reproduce result Example 2 Anderson et al. (2022), uses spending method 3c specified paper.","code":"set.seed(1234)  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Ex2 BH ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# # Transition matrix in Figure A2 m <- matrix(c(   0, 0.5, 0.5,   0.5, 0, 0.5,   0.5, 0.5, 0 ), nrow = 3, byrow = TRUE) # Initial weights w <- c(1 / 3, 1 / 3, 1 / 3)  # Event count of intersection of paired hypotheses - Table 2 event <- tribble(   ~H1, ~H2, ~Analysis, ~Event,   1, 1, 1, 155,   2, 2, 1, 160,   3, 3, 1, 165,   1, 2, 1, 85,   1, 3, 1, 85,   2, 3, 1, 85,   1, 1, 2, 305,   2, 2, 2, 320,   3, 3, 2, 335,   1, 2, 2, 170,   1, 3, 2, 170,   2, 3, 2, 170 )  event %>%   gt() %>%   tab_header(title = \"Event Count\") # Generate correlation from events corr <- generate_corr(event)  # Correlation matrix in Table 4 corr %>%   as_tibble() %>%   gt() %>%   fmt_number(columns = everything(), decimals = 2) %>%   tab_header(title = \"Correlation Matrix\") # WPGSD bounds, spending method 3c bound_WPGSD <- generate_bounds(   type = 3, k = 2, w = w, m = m, corr = corr, alpha = 0.025,   sf = list(sfLDOF, sfLDOF, sfLDOF),   sfparm = list(0, 0, 0),   t = list(c(155 / 305, 1), c(160 / 320, 1), c(165 / 335, 1)) )  # Bonferroni bounds bound_Bonf <- generate_bounds(   type = 0, k = 2, w = w, m = m, corr = corr, alpha = 0.025,   sf = list(sfLDOF, sfLDOF, sfLDOF),   sfparm = list(0, 0, 0),   t = list(c(155 / 305, 1), c(160 / 320, 1), c(165 / 335, 1)) )  bounds <- left_join(bound_Bonf, bound_WPGSD,   by = c(\"Hypotheses\", \"Analysis\"),   suffix = c(\".B\", \".W\") )  # Reorder for output bounds$order <- rep(c(5, 2, 1, 3, 6, 4, 7), 2) bounds <- bounds %>%   arrange(Analysis, order) %>%   select(-order)  # Table A6 bounds %>%   gt() %>%   fmt_number(columns = 3:9, decimals = 4) %>%   tab_header(title = \"Bonferroni and WPGSD Bounds\")"},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"power-considerations","dir":"Articles","previous_headings":"Methods and Examples","what":"Power Considerations","title":"Quickstart guide","text":"illustrates use WPGSD approach compute bounds analysis stage. design stage, one can take one following 2 options: 1) trial can first designed testing done weighted Bonferroni conservative sample size estimate. analysis stage, correlation can taken consideration WPGSD approach bound calculation; 2) adjust sample size downward using WPGSD approach design stage, one can power study taking minimum \\(p\\)-value bound given individual hypothesis WPGSD table (assumed correlation structure). example, \\(H_2\\) example 1, \\(\\hbox{min}(0.0011,0.0017,0.0010,0.0030)=0.0010\\) \\(k=1\\) \\(\\hbox{min}(0.0092,0.0144,0.0081,0.0238)=0.0081\\) \\(k=2\\). \\(H_2\\) bounds 0.0010 (\\(k=1\\)) 0.0081 (\\(k=2\\)) can used power \\(H_2\\). R function 2nd option development.","code":""},{"path":"https://merck.github.io/wpgsd/articles/wpgsd.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"Quickstart guide","text":"WPGSD approach provides unification previous work parametric testing group sequential design. enabled complex scenarios requires attention consonance intersection hypotheses. Although detailed closed testing required, deterrent. approach accommodates various spending approaches provides relaxed bounds improved power compared Bonferroni approach.","code":""},{"path":[]},{"path":"https://merck.github.io/wpgsd/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Keaven Anderson. Author. Zifang Guo. Author. Jing Zhao. Author. Linda Sun. Author. Yi Cui. Author. Yujie Zhao. Author, maintainer. Larry Leon. Author. Merck Sharp & Dohme Corp. Copyright holder.","code":""},{"path":"https://merck.github.io/wpgsd/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Anderson KM, Guo Z, Zhao J, Sun LZ (2022). “unified framework weighted parametric group sequential design.” Biometrical Journal, 64(7), 1219–1239. doi:10.1002/bimj.202100085.","code":"@Article{,   title = {A unified framework for weighted parametric group sequential design},   author = {Keaven M Anderson and Zifang Guo and Jing Zhao and Linda Z Sun},   journal = {Biometrical Journal},   volume = {64},   number = {7},   pages = {1219--1239},   year = {2022},   publisher = {Wiley Online Library},   doi = {10.1002/bimj.202100085}, }"},{"path":"https://merck.github.io/wpgsd/index.html","id":"wpgsd-","dir":"","previous_headings":"","what":"Weighted Parametric Group Sequential Design","title":"Weighted Parametric Group Sequential Design","text":"Weighted parametric group sequential design (WPGSD) allows one take advantage known correlation structure constructing efficacy bounds control family-wise error rate (FWER) group sequential design. correlation may due common observations nested populations, due common observations overlapping populations, due common observations control arm.","code":""},{"path":"https://merck.github.io/wpgsd/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Weighted Parametric Group Sequential Design","text":"Install development version wpgsd GitHub:","code":"remotes::install_github(\"Merck/wpgsd\")"},{"path":"https://merck.github.io/wpgsd/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Weighted Parametric Group Sequential Design","text":"Anderson, K. M., Guo, Z., Zhao, J., & Sun, L. Z. (2022). unified framework weighted parametric group sequential design. Biometrical Journal, 64(7), 1219–1239. BibTeX entry:","code":"@article{anderson2022unified,   title     = {A unified framework for weighted parametric group sequential design},   author    = {Anderson, Keaven M and Guo, Zifang and Zhao, Jing and Sun, Linda Z},   journal   = {Biometrical Journal},   volume    = {64},   number    = {7},   pages     = {1219--1239},   year      = {2022},   publisher = {Wiley Online Library} }"},{"path":"https://merck.github.io/wpgsd/reference/calc_seq_p.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate sequential p-values for interaction/elementary hypothesis — calc_seq_p","title":"Calculate sequential p-values for interaction/elementary hypothesis — calc_seq_p","text":"Calculate sequential p-values interaction/elementary hypothesis","code":""},{"path":"https://merck.github.io/wpgsd/reference/calc_seq_p.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate sequential p-values for interaction/elementary hypothesis — calc_seq_p","text":"","code":"calc_seq_p(   test_analysis = 1,   test_hypothesis = \"H1, H2, H3\",   p_obs,   alpha_spending_type = 3,   n_analysis = 2,   initial_weight,   transition_mat,   z_corr,   spending_fun,   spending_fun_par,   info_frac,   interval = c(1e-04, 0.2) )"},{"path":"https://merck.github.io/wpgsd/reference/calc_seq_p.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate sequential p-values for interaction/elementary hypothesis — calc_seq_p","text":"test_analysis index analysis tested, 1, 2, ... test_hypothesis character tested interaction/elementary hypothesis, \"H1, H2, H3\", H1, H2, \"H1\". p_obs Observed p-values. alpha_spending_type Type Boundary type. 0 - Bonferroni. Separate alpha spending hypotheses. 1 - Fixed alpha spending hypotheses. Method 3a manuscript. 2 - Overall alpha spending hypotheses. Method 3b manuscript. 3 - Separate alpha spending hypotheses. Method 3c manuscript. n_analysis Total number analysis. initial_weight Initial weight assigned elementary hypothesis. transition_mat Transition matrix. z_corr Correlation matrix Z statistics. spending_fun Spending function. spending_fun_par Parameter spending function. info_frac Information fractions. interval Interval search uniroot.","code":""},{"path":"https://merck.github.io/wpgsd/reference/calc_seq_p.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate sequential p-values for interaction/elementary hypothesis — calc_seq_p","text":"sequential p-values test_hypothesis test_analysis.","code":""},{"path":"https://merck.github.io/wpgsd/reference/calc_seq_p.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate sequential p-values for interaction/elementary hypothesis — calc_seq_p","text":"","code":"p_obs <- dplyr::bind_rows(   tibble::tibble(Analysis = 1, H1 = 0.001, H2 = 0.001),   tibble::tibble(Analysis = 2, H1 = 0.001, H2 = 0.001) ) bound <- tibble::tribble(   ~Analysis, ~Hypotheses, ~H1, ~H2,   1, \"H1\", 0.02, NA,   1, \"H1, H2\", 0.0001, 0.00001,   1, \"H2\", NA, 0.003,   2, \"H1\", 0.02, NA,   2, \"H1, H2\", 0.02, 0.00001,   2, \"H2\", NA, 0.003 )  closed_test <- closed_test(bound, p_obs)"},{"path":"https://merck.github.io/wpgsd/reference/closed_test.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform closed testing procedure — closed_test","title":"Perform closed testing procedure — closed_test","text":"Perform closed testing procedure","code":""},{"path":"https://merck.github.io/wpgsd/reference/closed_test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform closed testing procedure — closed_test","text":"","code":"closed_test(bounds, p_obs)"},{"path":"https://merck.github.io/wpgsd/reference/closed_test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform closed testing procedure — closed_test","text":"bounds tibble nominal p-value boundaries generate_bounds() containing columns Analysis, Hypotheses, H1, H2, etc. p_obs tibble observed p-values containing columns Analysis, H1, H2, etc.","code":""},{"path":"https://merck.github.io/wpgsd/reference/closed_test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform closed testing procedure — closed_test","text":"outcome matrix summarizing testing results.","code":""},{"path":"https://merck.github.io/wpgsd/reference/closed_test.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform closed testing procedure — closed_test","text":"","code":"p_obs <- dplyr::bind_rows(   tibble::tibble(Analysis = 1, H1 = 0.001, H2 = 0.001),   tibble::tibble(Analysis = 2, H1 = 0.001, H2 = 0.001) ) bound <- tibble::tribble(   ~Analysis, ~Hypotheses, ~H1, ~H2,   1, \"H1\", 0.02, NA,   1, \"H1, H2\", 0.0001, 0.00001,   1, \"H2\", NA, 0.003,   2, \"H1\", 0.02, NA,   2, \"H1, H2\", 0.02, 0.00001,   2, \"H2\", NA, 0.003 )  closed_test <- closed_test(bound, p_obs)"},{"path":"https://merck.github.io/wpgsd/reference/find_astar.html","id":null,"dir":"Reference","previous_headings":"","what":"Utility function for root-finding to compute crossing probabilities\nwith the overall alpha spending approach — find_astar","title":"Utility function for root-finding to compute crossing probabilities\nwith the overall alpha spending approach — find_astar","text":"Utility function root-finding compute crossing probabilities overall alpha spending approach","code":""},{"path":"https://merck.github.io/wpgsd/reference/find_astar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Utility function for root-finding to compute crossing probabilities\nwith the overall alpha spending approach — find_astar","text":"","code":"find_astar(   a,   alpha_prev = NULL,   astar,   w,   sig,   maxpts = 50000,   abseps = 1e-05,   ... )"},{"path":"https://merck.github.io/wpgsd/reference/find_astar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Utility function for root-finding to compute crossing probabilities\nwith the overall alpha spending approach — find_astar","text":"Cumulative overall alpha spending current analysis. alpha_prev alpha boundary previous interim analyses using WPGSD approach. astar Total nominal alpha level current analysis WPGSD approach. w Vector alpha weights current analysis. sig Correlation matrix previous current analyses test statistics. maxpts GenzBretz function maximum number function values integer. abseps GenzBretz function absolute error tolerance. ... Additional arguments.","code":""},{"path":"https://merck.github.io/wpgsd/reference/find_astar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Utility function for root-finding to compute crossing probabilities\nwith the overall alpha spending approach — find_astar","text":"Difference. 0 astar identified.","code":""},{"path":"https://merck.github.io/wpgsd/reference/find_astar.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Utility function for root-finding to compute crossing probabilities\nwith the overall alpha spending approach — find_astar","text":"","code":"# Input event count of intersection of paired hypotheses - Table 2 my_event <- tibble::tribble(   ~H1, ~H2, ~Analysis, ~Event,   1, 1, 1, 155,   2, 2, 1, 160,   3, 3, 1, 165,   1, 2, 1, 85,   1, 3, 1, 85,   2, 3, 1, 85,   1, 1, 2, 305,   2, 2, 2, 320,   3, 3, 2, 335,   1, 2, 2, 170,   1, 3, 2, 170,   2, 3, 2, 170 )  # Generate correlation from events my_corr <- generate_corr(my_event)  # Find the inflation factor for H1, H2 at analysis 1 find_astar(   a = 0.0008708433,   alpha_prev = NULL,   aprime = c(0.0004588644, 0.0004119789),   astar = 1,   w = c(0.5, 0.5),   sig = my_corr[     colnames(my_corr) %in% c(\"H1_A1\", \"H2_A1\"),     colnames(my_corr) %in% c(\"H1_A1\", \"H2_A1\")   ] ) #> [1] 0.6583884 #> attr(,\"error\") #> [1] 1e-15 #> attr(,\"msg\") #> [1] \"Normal Completion\""},{"path":"https://merck.github.io/wpgsd/reference/find_xi.html","id":null,"dir":"Reference","previous_headings":"","what":"Utility function for root-finding to compute inflation factor xi\nwith the separate alpha spending approach — find_xi","title":"Utility function for root-finding to compute inflation factor xi\nwith the separate alpha spending approach — find_xi","text":"Utility function root-finding compute inflation factor xi separate alpha spending approach","code":""},{"path":"https://merck.github.io/wpgsd/reference/find_xi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Utility function for root-finding to compute inflation factor xi\nwith the separate alpha spending approach — find_xi","text":"","code":"find_xi(   a,   alpha_prev = NULL,   aprime,   xi,   sig,   maxpts = 50000,   abseps = 1e-05,   ... )"},{"path":"https://merck.github.io/wpgsd/reference/find_xi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Utility function for root-finding to compute inflation factor xi\nwith the separate alpha spending approach — find_xi","text":"Sum cumulative alpha spending Bonferroni approach. alpha_prev alpha boundary previous interim analyses using MTP approach. aprime Nominal alpha boundary Bonferroni approach. xi Inflation factor. sig Correlation matrix previous current analyses test statistics. maxpts GenzBretz function maximum number function values integer. abseps GenzBretz function absolute error tolerance. ... Additional arguments.","code":""},{"path":"https://merck.github.io/wpgsd/reference/find_xi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Utility function for root-finding to compute inflation factor xi\nwith the separate alpha spending approach — find_xi","text":"Difference. 0 xi identified.","code":""},{"path":"https://merck.github.io/wpgsd/reference/find_xi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Utility function for root-finding to compute inflation factor xi\nwith the separate alpha spending approach — find_xi","text":"","code":"# Input event count of intersection of paired hypotheses - Table 2 my_event <- tibble::tribble(   ~H1, ~H2, ~Analysis, ~Event,   1, 1, 1, 155,   2, 2, 1, 160,   3, 3, 1, 165,   1, 2, 1, 85,   1, 3, 1, 85,   2, 3, 1, 85,   1, 1, 2, 305,   2, 2, 2, 320,   3, 3, 2, 335,   1, 2, 2, 170,   1, 3, 2, 170,   2, 3, 2, 170 )  # Generate correlation from events my_corr <- generate_corr(my_event)  # Find the inflation factor for H1, H2 at analysis 1 find_xi(   a = 0.0008708433,   alpha_prev = NULL,   aprime = c(0.0004588644, 0.0004119789),   xi = 1,   sig = my_corr[     colnames(my_corr) %in% c(\"H1_A1\", \"H2_A1\"),     colnames(my_corr) %in% c(\"H1_A1\", \"H2_A1\")   ] ) #> [1] -2.237679e-05 #> attr(,\"error\") #> [1] 1e-15 #> attr(,\"msg\") #> [1] \"Normal Completion\""},{"path":"https://merck.github.io/wpgsd/reference/generate_bounds.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute p-value boundaries of the parametric MTP method with overall\nalpha spending for all hypotheses — generate_bounds","title":"Compute p-value boundaries of the parametric MTP method with overall\nalpha spending for all hypotheses — generate_bounds","text":"Compute p-value boundaries parametric MTP method overall alpha spending hypotheses","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_bounds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute p-value boundaries of the parametric MTP method with overall\nalpha spending for all hypotheses — generate_bounds","text":"","code":"generate_bounds(   type = 1,   k = 2,   w = w,   m = m,   corr = corr,   alpha = 0.025,   cum_alpha = NULL,   maxpts = 50000,   abseps = 1e-05,   tol = 1e-10,   sf = gsDesign::sfHSD,   sfparm = -4,   t = c(0.5, 1),   ... )"},{"path":"https://merck.github.io/wpgsd/reference/generate_bounds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute p-value boundaries of the parametric MTP method with overall\nalpha spending for all hypotheses — generate_bounds","text":"type Boundary type. 0 = Bonferroni. Separate alpha spending hypotheses. 1 = Fixed alpha spending hypotheses. Method 3a manuscript. 2 = Overall alpha spending hypotheses. Method 3b manuscript. 3 = Separate alpha spending hypotheses. Method 3c manuscript. k Number analyses current analysis. w Initial weights. m Transition matrix. corr Correlation matrix test statistics current analysis. dim = k * length(w). alpha Overall alpha. cum_alpha Cumulative alpha spent analysis. required type = 1. maxpts GenzBretz function maximum number function values integer. abseps GenzBretz function absolute error tolerance. tol Find root tolerance. sf list alpha spending functions spend alpha hypotheses. type = 0 3 length equals number hypotheses. type = 1 sf needed. type = 2 first component used. sfparm list parameters supplied sfs. type = 0 3 length equals number hypotheses. type = 1 sfparm needed. type = 2 first component used. t list information fraction used alpha spending, may different actual information fraction. component corresponds hypothesis. type = 0 3 length equals number hypotheses. type = 1 t needed. type = 2 first component used. ... Additional arguments.","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_bounds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute p-value boundaries of the parametric MTP method with overall\nalpha spending for all hypotheses — generate_bounds","text":"tibble k * (2^(n_hypotheses - 1)) rows p-value boundaries. Inflation factor also provided type = 3.","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_bounds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute p-value boundaries of the parametric MTP method with overall\nalpha spending for all hypotheses — generate_bounds","text":"","code":"# Build the transition matrix m <- matrix(c(   0, 0.5, 0.5,   0.5, 0, 0.5,   0.5, 0.5, 0 ), nrow = 3, byrow = TRUE)  # Initialize weights w <- c(1 / 3, 1 / 3, 1 / 3)  # Input information fraction IF_IA <- c(155 / 305, 160 / 320, 165 / 335)  # Input event count of intersection of paired hypotheses - Table 2 event <- tibble::tribble(   ~H1, ~H2, ~Analysis, ~Event,   1, 1, 1, 155,   2, 2, 1, 160,   3, 3, 1, 165,   1, 2, 1, 85,   1, 3, 1, 85,   2, 3, 1, 85,   1, 1, 2, 305,   2, 2, 2, 320,   3, 3, 2, 335,   1, 2, 2, 170,   1, 3, 2, 170,   2, 3, 2, 170 )  # Generate correlation from events gs_corr <- generate_corr(event)  # Generate bounds generate_bounds(   type = 3,   k = 2,   w = w,   m = m,   corr = gs_corr,   alpha = 0.025,   sf = list(gsDesign::sfLDOF, gsDesign::sfLDOF, gsDesign::sfLDOF),   sfparm = list(0, 0, 0),   t = list(c(IF_IA[1], 1), c(IF_IA[2], 1), c(IF_IA[3], 1)) ) #> # A tibble: 14 × 6 #>    Analysis Hypotheses        H1        H2        H3    xi #>       <int> <chr>          <dbl>     <dbl>     <dbl> <dbl> #>  1        1 H1          0.00167  NA        NA         1    #>  2        1 H1, H2      0.000471  0.000423 NA         1.03 #>  3        1 H1, H2, H3  0.000224  0.000199  0.000178  1.04 #>  4        1 H1, H3      0.000470 NA         0.000382  1.02 #>  5        1 H2         NA         0.00153  NA         1    #>  6        1 H2, H3     NA         0.000421  0.000381  1.02 #>  7        1 H3         NA        NA         0.00140   1    #>  8        2 H1          0.0245   NA        NA         1    #>  9        2 H1, H2      0.0135    0.0135   NA         1.09 #> 10        2 H1, H2, H3  0.00949   0.00950   0.00951   1.15 #> 11        2 H1, H3      0.0135   NA         0.0135    1.09 #> 12        2 H2         NA         0.0245   NA         1    #> 13        2 H2, H3     NA         0.0134    0.0134    1.09 #> 14        2 H3         NA        NA         0.0245    1"},{"path":"https://merck.github.io/wpgsd/reference/generate_corr.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate correlation matrix based on event counts — generate_corr","title":"Generate correlation matrix based on event counts — generate_corr","text":"Generate correlation matrix based event counts","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_corr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate correlation matrix based on event counts — generate_corr","text":"","code":"generate_corr(event)"},{"path":"https://merck.github.io/wpgsd/reference/generate_corr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate correlation matrix based on event counts — generate_corr","text":"event Event count hypothesis analysis, including event count intersection hypotheses. contains 4 columns: H1, H2, Analysis, Event. H1 needs listed 1, 2, 3, etc. numbers.","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_corr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate correlation matrix based on event counts — generate_corr","text":"correlation matrix.","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_corr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate correlation matrix based on event counts — generate_corr","text":"","code":"# Build the transition matrix m <- matrix(c(   0, 0.5, 0.5,   0.5, 0, 0.5,   0.5, 0.5, 0 ), nrow = 3, byrow = TRUE) # initialize weights w <- c(1 / 3, 1 / 3, 1 / 3)  # Input event count of intersection of paired hypotheses - Table 2 event <- tibble::tribble(   ~H1, ~H2, ~Analysis, ~Event,   1, 1, 1, 155,   2, 2, 1, 160,   3, 3, 1, 165,   1, 2, 1, 85,   1, 3, 1, 85,   2, 3, 1, 85,   1, 1, 2, 305,   2, 2, 2, 320,   3, 3, 2, 335,   1, 2, 2, 170,   1, 3, 2, 170,   2, 3, 2, 170 )  # Generate correlation from events gs_corr <- generate_corr(event)"},{"path":"https://merck.github.io/wpgsd/reference/generate_event_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate table of event counts from ADSL and ADTTE datasets — generate_event_table","title":"Generate table of event counts from ADSL and ADTTE datasets — generate_event_table","text":"Generate table event counts ADSL ADTTE datasets","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_event_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate table of event counts from ADSL and ADTTE datasets — generate_event_table","text":"","code":"generate_event_table(paths, h_select, adsl_name, adtte_name, key_var, cnsr_var)"},{"path":"https://merck.github.io/wpgsd/reference/generate_event_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate table of event counts from ADSL and ADTTE datasets — generate_event_table","text":"paths vector paths analysis datasets. Length equal number analyses completed. h_select Selection criterion hypothesis. tibble containing 2 columns: Hypothesis Crit. adsl_name SAS dataset name subject-level analysis data. Usually \"adsl\". adtte_name SAS dataset name time--event analysis data. Usually \"adtte\". key_var Key variable join adsl adtte datasets. example, \"USUBJID\" \"SUBJID\". cnsr_var Variable indicate censoring (1 = censor; 0 = event). example, \"CNSR\".","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_event_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate table of event counts from ADSL and ADTTE datasets — generate_event_table","text":"list two components: event: event count table input generate_bounds(). dsets: analysis datasets hypothesis.","code":""},{"path":"https://merck.github.io/wpgsd/reference/generate_event_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate table of event counts from ADSL and ADTTE datasets — generate_event_table","text":"","code":"library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union  paths <- system.file(\"extdata/\", package = \"wpgsd\")  # Selection criteria for each hypothesis h_select <- tibble::tribble(   ~Hypothesis, ~Crit,   1, \"PARAMCD == 'OS' & TRT01P %in% c('Xanomeline High Dose', 'Placebo')\",   2, \"PARAMCD == 'OS' & TRT01P %in% c('Xanomeline Low Dose', 'Placebo')\" )  event <- generate_event_table(paths, h_select,   adsl_name = \"adsl\", adtte_name = \"adtte\",   key_var = \"USUBJID\", cnsr_var = \"CNSR\" )$event  event %>%   gt::gt() %>%   gt::tab_header(title = \"Event Count - Computed from SAS Datasets Example\") #> <div id=\"cnvtmyvzmd\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\"> #>   <style>#cnvtmyvzmd table { #>   font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; #>   -webkit-font-smoothing: antialiased; #>   -moz-osx-font-smoothing: grayscale; #> } #>  #> #cnvtmyvzmd thead, #cnvtmyvzmd tbody, #cnvtmyvzmd tfoot, #cnvtmyvzmd tr, #cnvtmyvzmd td, #cnvtmyvzmd th { #>   border-style: none; #> } #>  #> #cnvtmyvzmd p { #>   margin: 0; #>   padding: 0; #> } #>  #> #cnvtmyvzmd .gt_table { #>   display: table; #>   border-collapse: collapse; #>   line-height: normal; #>   margin-left: auto; #>   margin-right: auto; #>   color: #333333; #>   font-size: 16px; #>   font-weight: normal; #>   font-style: normal; #>   background-color: #FFFFFF; #>   width: auto; #>   border-top-style: solid; #>   border-top-width: 2px; #>   border-top-color: #A8A8A8; #>   border-right-style: none; #>   border-right-width: 2px; #>   border-right-color: #D3D3D3; #>   border-bottom-style: solid; #>   border-bottom-width: 2px; #>   border-bottom-color: #A8A8A8; #>   border-left-style: none; #>   border-left-width: 2px; #>   border-left-color: #D3D3D3; #> } #>  #> #cnvtmyvzmd .gt_caption { #>   padding-top: 4px; #>   padding-bottom: 4px; #> } #>  #> #cnvtmyvzmd .gt_title { #>   color: #333333; #>   font-size: 125%; #>   font-weight: initial; #>   padding-top: 4px; #>   padding-bottom: 4px; #>   padding-left: 5px; #>   padding-right: 5px; #>   border-bottom-color: #FFFFFF; #>   border-bottom-width: 0; #> } #>  #> #cnvtmyvzmd .gt_subtitle { #>   color: #333333; #>   font-size: 85%; #>   font-weight: initial; #>   padding-top: 3px; #>   padding-bottom: 5px; #>   padding-left: 5px; #>   padding-right: 5px; #>   border-top-color: #FFFFFF; #>   border-top-width: 0; #> } #>  #> #cnvtmyvzmd .gt_heading { #>   background-color: #FFFFFF; #>   text-align: center; #>   border-bottom-color: #FFFFFF; #>   border-left-style: none; #>   border-left-width: 1px; #>   border-left-color: #D3D3D3; #>   border-right-style: none; #>   border-right-width: 1px; #>   border-right-color: #D3D3D3; #> } #>  #> #cnvtmyvzmd .gt_bottom_border { #>   border-bottom-style: solid; #>   border-bottom-width: 2px; #>   border-bottom-color: #D3D3D3; #> } #>  #> #cnvtmyvzmd .gt_col_headings { #>   border-top-style: solid; #>   border-top-width: 2px; #>   border-top-color: #D3D3D3; #>   border-bottom-style: solid; #>   border-bottom-width: 2px; #>   border-bottom-color: #D3D3D3; #>   border-left-style: none; #>   border-left-width: 1px; #>   border-left-color: #D3D3D3; #>   border-right-style: none; #>   border-right-width: 1px; #>   border-right-color: #D3D3D3; #> } #>  #> #cnvtmyvzmd .gt_col_heading { #>   color: #333333; #>   background-color: #FFFFFF; #>   font-size: 100%; #>   font-weight: normal; #>   text-transform: inherit; #>   border-left-style: none; #>   border-left-width: 1px; #>   border-left-color: #D3D3D3; #>   border-right-style: none; #>   border-right-width: 1px; #>   border-right-color: #D3D3D3; #>   vertical-align: bottom; #>   padding-top: 5px; #>   padding-bottom: 6px; #>   padding-left: 5px; #>   padding-right: 5px; #>   overflow-x: hidden; #> } #>  #> #cnvtmyvzmd .gt_column_spanner_outer { #>   color: #333333; #>   background-color: #FFFFFF; #>   font-size: 100%; #>   font-weight: normal; #>   text-transform: inherit; #>   padding-top: 0; #>   padding-bottom: 0; #>   padding-left: 4px; #>   padding-right: 4px; #> } #>  #> #cnvtmyvzmd .gt_column_spanner_outer:first-child { #>   padding-left: 0; #> } #>  #> #cnvtmyvzmd .gt_column_spanner_outer:last-child { #>   padding-right: 0; #> } #>  #> #cnvtmyvzmd .gt_column_spanner { #>   border-bottom-style: solid; #>   border-bottom-width: 2px; #>   border-bottom-color: #D3D3D3; #>   vertical-align: bottom; #>   padding-top: 5px; #>   padding-bottom: 5px; #>   overflow-x: hidden; #>   display: inline-block; #>   width: 100%; #> } #>  #> #cnvtmyvzmd .gt_spanner_row { #>   border-bottom-style: hidden; #> } #>  #> #cnvtmyvzmd .gt_group_heading { #>   padding-top: 8px; #>   padding-bottom: 8px; #>   padding-left: 5px; #>   padding-right: 5px; #>   color: #333333; #>   background-color: #FFFFFF; #>   font-size: 100%; #>   font-weight: initial; #>   text-transform: inherit; #>   border-top-style: solid; #>   border-top-width: 2px; #>   border-top-color: #D3D3D3; #>   border-bottom-style: solid; #>   border-bottom-width: 2px; #>   border-bottom-color: #D3D3D3; #>   border-left-style: none; #>   border-left-width: 1px; #>   border-left-color: #D3D3D3; #>   border-right-style: none; #>   border-right-width: 1px; #>   border-right-color: #D3D3D3; #>   vertical-align: middle; #>   text-align: left; #> } #>  #> #cnvtmyvzmd .gt_empty_group_heading { #>   padding: 0.5px; #>   color: #333333; #>   background-color: #FFFFFF; #>   font-size: 100%; #>   font-weight: initial; #>   border-top-style: solid; #>   border-top-width: 2px; #>   border-top-color: #D3D3D3; #>   border-bottom-style: solid; #>   border-bottom-width: 2px; #>   border-bottom-color: #D3D3D3; #>   vertical-align: middle; #> } #>  #> #cnvtmyvzmd .gt_from_md > :first-child { #>   margin-top: 0; #> } #>  #> #cnvtmyvzmd .gt_from_md > :last-child { #>   margin-bottom: 0; #> } #>  #> #cnvtmyvzmd .gt_row { #>   padding-top: 8px; #>   padding-bottom: 8px; #>   padding-left: 5px; #>   padding-right: 5px; #>   margin: 10px; #>   border-top-style: solid; #>   border-top-width: 1px; #>   border-top-color: #D3D3D3; #>   border-left-style: none; #>   border-left-width: 1px; #>   border-left-color: #D3D3D3; #>   border-right-style: none; #>   border-right-width: 1px; #>   border-right-color: #D3D3D3; #>   vertical-align: middle; #>   overflow-x: hidden; #> } #>  #> #cnvtmyvzmd .gt_stub { #>   color: #333333; #>   background-color: #FFFFFF; #>   font-size: 100%; #>   font-weight: initial; #>   text-transform: inherit; #>   border-right-style: solid; #>   border-right-width: 2px; #>   border-right-color: #D3D3D3; #>   padding-left: 5px; #>   padding-right: 5px; #> } #>  #> #cnvtmyvzmd .gt_stub_row_group { #>   color: #333333; #>   background-color: #FFFFFF; #>   font-size: 100%; #>   font-weight: initial; #>   text-transform: inherit; #>   border-right-style: solid; #>   border-right-width: 2px; #>   border-right-color: #D3D3D3; #>   padding-left: 5px; #>   padding-right: 5px; #>   vertical-align: top; #> } #>  #> #cnvtmyvzmd .gt_row_group_first td { #>   border-top-width: 2px; #> } #>  #> #cnvtmyvzmd .gt_row_group_first th { #>   border-top-width: 2px; #> } #>  #> #cnvtmyvzmd .gt_summary_row { #>   color: #333333; #>   background-color: #FFFFFF; #>   text-transform: inherit; #>   padding-top: 8px; #>   padding-bottom: 8px; #>   padding-left: 5px; #>   padding-right: 5px; #> } #>  #> #cnvtmyvzmd .gt_first_summary_row { #>   border-top-style: solid; #>   border-top-color: #D3D3D3; #> } #>  #> #cnvtmyvzmd .gt_first_summary_row.thick { #>   border-top-width: 2px; #> } #>  #> #cnvtmyvzmd .gt_last_summary_row { #>   padding-top: 8px; #>   padding-bottom: 8px; #>   padding-left: 5px; #>   padding-right: 5px; #>   border-bottom-style: solid; #>   border-bottom-width: 2px; #>   border-bottom-color: #D3D3D3; #> } #>  #> #cnvtmyvzmd .gt_grand_summary_row { #>   color: #333333; #>   background-color: #FFFFFF; #>   text-transform: inherit; #>   padding-top: 8px; #>   padding-bottom: 8px; #>   padding-left: 5px; #>   padding-right: 5px; #> } #>  #> #cnvtmyvzmd .gt_first_grand_summary_row { #>   padding-top: 8px; #>   padding-bottom: 8px; #>   padding-left: 5px; #>   padding-right: 5px; #>   border-top-style: double; #>   border-top-width: 6px; #>   border-top-color: #D3D3D3; #> } #>  #> #cnvtmyvzmd .gt_last_grand_summary_row_top { #>   padding-top: 8px; #>   padding-bottom: 8px; #>   padding-left: 5px; #>   padding-right: 5px; #>   border-bottom-style: double; #>   border-bottom-width: 6px; #>   border-bottom-color: #D3D3D3; #> } #>  #> #cnvtmyvzmd .gt_striped { #>   background-color: rgba(128, 128, 128, 0.05); #> } #>  #> #cnvtmyvzmd .gt_table_body { #>   border-top-style: solid; #>   border-top-width: 2px; #>   border-top-color: #D3D3D3; #>   border-bottom-style: solid; #>   border-bottom-width: 2px; #>   border-bottom-color: #D3D3D3; #> } #>  #> #cnvtmyvzmd .gt_footnotes { #>   color: #333333; #>   background-color: #FFFFFF; #>   border-bottom-style: none; #>   border-bottom-width: 2px; #>   border-bottom-color: #D3D3D3; #>   border-left-style: none; #>   border-left-width: 2px; #>   border-left-color: #D3D3D3; #>   border-right-style: none; #>   border-right-width: 2px; #>   border-right-color: #D3D3D3; #> } #>  #> #cnvtmyvzmd .gt_footnote { #>   margin: 0px; #>   font-size: 90%; #>   padding-top: 4px; #>   padding-bottom: 4px; #>   padding-left: 5px; #>   padding-right: 5px; #> } #>  #> #cnvtmyvzmd .gt_sourcenotes { #>   color: #333333; #>   background-color: #FFFFFF; #>   border-bottom-style: none; #>   border-bottom-width: 2px; #>   border-bottom-color: #D3D3D3; #>   border-left-style: none; #>   border-left-width: 2px; #>   border-left-color: #D3D3D3; #>   border-right-style: none; #>   border-right-width: 2px; #>   border-right-color: #D3D3D3; #> } #>  #> #cnvtmyvzmd .gt_sourcenote { #>   font-size: 90%; #>   padding-top: 4px; #>   padding-bottom: 4px; #>   padding-left: 5px; #>   padding-right: 5px; #> } #>  #> #cnvtmyvzmd .gt_left { #>   text-align: left; #> } #>  #> #cnvtmyvzmd .gt_center { #>   text-align: center; #> } #>  #> #cnvtmyvzmd .gt_right { #>   text-align: right; #>   font-variant-numeric: tabular-nums; #> } #>  #> #cnvtmyvzmd .gt_font_normal { #>   font-weight: normal; #> } #>  #> #cnvtmyvzmd .gt_font_bold { #>   font-weight: bold; #> } #>  #> #cnvtmyvzmd .gt_font_italic { #>   font-style: italic; #> } #>  #> #cnvtmyvzmd .gt_super { #>   font-size: 65%; #> } #>  #> #cnvtmyvzmd .gt_footnote_marks { #>   font-size: 75%; #>   vertical-align: 0.4em; #>   position: initial; #> } #>  #> #cnvtmyvzmd .gt_asterisk { #>   font-size: 100%; #>   vertical-align: 0; #> } #>  #> #cnvtmyvzmd .gt_indent_1 { #>   text-indent: 5px; #> } #>  #> #cnvtmyvzmd .gt_indent_2 { #>   text-indent: 10px; #> } #>  #> #cnvtmyvzmd .gt_indent_3 { #>   text-indent: 15px; #> } #>  #> #cnvtmyvzmd .gt_indent_4 { #>   text-indent: 20px; #> } #>  #> #cnvtmyvzmd .gt_indent_5 { #>   text-indent: 25px; #> } #> <\/style> #>   <table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\"> #>   <thead> #>     <tr class=\"gt_heading\"> #>       <td colspan=\"4\" class=\"gt_heading gt_title gt_font_normal gt_bottom_border\" style>Event Count - Computed from SAS Datasets Example<\/td> #>     <\/tr> #>      #>     <tr class=\"gt_col_headings\"> #>       <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"H1\">H1<\/th> #>       <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"H2\">H2<\/th> #>       <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Analysis\">Analysis<\/th> #>       <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Event\">Event<\/th> #>     <\/tr> #>   <\/thead> #>   <tbody class=\"gt_table_body\"> #>     <tr><td headers=\"H1\" class=\"gt_row gt_right\">1<\/td> #> <td headers=\"H2\" class=\"gt_row gt_right\">1<\/td> #> <td headers=\"Analysis\" class=\"gt_row gt_right\">1<\/td> #> <td headers=\"Event\" class=\"gt_row gt_right\">66<\/td><\/tr> #>     <tr><td headers=\"H1\" class=\"gt_row gt_right\">2<\/td> #> <td headers=\"H2\" class=\"gt_row gt_right\">2<\/td> #> <td headers=\"Analysis\" class=\"gt_row gt_right\">1<\/td> #> <td headers=\"Event\" class=\"gt_row gt_right\">59<\/td><\/tr> #>     <tr><td headers=\"H1\" class=\"gt_row gt_right\">1<\/td> #> <td headers=\"H2\" class=\"gt_row gt_right\">2<\/td> #> <td headers=\"Analysis\" class=\"gt_row gt_right\">1<\/td> #> <td headers=\"Event\" class=\"gt_row gt_right\">45<\/td><\/tr> #>   <\/tbody> #>    #>    #> <\/table> #> <\/div>"},{"path":"https://merck.github.io/wpgsd/reference/tidyeval.html","id":null,"dir":"Reference","previous_headings":"","what":"Tidy eval helpers — tidyeval","title":"Tidy eval helpers — tidyeval","text":"page lists tidy eval tools reexported package rlang. learn using tidy eval scripts packages high level, see dplyr programming vignette ggplot2 packages vignette. Metaprogramming section Advanced R may also useful deeper dive. tidy eval operators {{, !!, !!! syntactic constructs specially interpreted tidy eval functions. mostly need {{, !! !!! advanced operators use simple cases. curly-curly operator {{ allows tunnel data-variables passed function arguments inside tidy eval functions. {{ designed individual arguments. pass multiple arguments contained dots, use ... normal way.   enquo() enquos() delay execution one several function arguments. former returns single expression, latter returns list expressions. defused, expressions longer evaluate . must injected back evaluation context !! (single expression) !!! (list expressions).   simple case, code equivalent usage {{ ... . Defusing enquo() enquos() needed complex cases, instance need inspect modify expressions way. .data pronoun object represents current slice data. variable name string, use .data pronoun subset variable [[.   Another tidy eval operator :=. makes possible use glue curly-curly syntax LHS =. technical reasons, R language support complex expressions left =, use := workaround.   Many tidy eval functions like dplyr::mutate() dplyr::summarise() give automatic name unnamed inputs. need create sort automatic names , use as_label(). instance, glue-tunnelling syntax can reproduced manually :   Expressions defused enquo() (tunnelled {{) need simple column names, can arbitrarily complex. as_label() handles cases gracefully. code assumes simple column name, use as_name() instead. safer throws error input name expected.","code":"my_function <- function(data, var, ...) {   data %>%     group_by(...) %>%     summarise(mean = mean({{ var }})) } my_function <- function(data, var, ...) {   # Defuse   var <- enquo(var)   dots <- enquos(...)    # Inject   data %>%     group_by(!!!dots) %>%     summarise(mean = mean(!!var)) } my_var <- \"disp\" mtcars %>% summarise(mean = mean(.data[[my_var]])) my_function <- function(data, var, suffix = \"foo\") {   # Use `{{` to tunnel function arguments and the usual glue   # operator `{` to interpolate plain strings.   data %>%     summarise(\"{{ var }}_mean_{suffix}\" := mean({{ var }})) } my_function <- function(data, var, suffix = \"foo\") {   var <- enquo(var)   prefix <- as_label(var)   data %>%     summarise(\"{prefix}_mean_{suffix}\" := mean(!!var)) }"},{"path":"https://merck.github.io/wpgsd/reference/wpgsd-package.html","id":null,"dir":"Reference","previous_headings":"","what":"wpgsd: Weighted Parametric Group Sequential Design — wpgsd-package","title":"wpgsd: Weighted Parametric Group Sequential Design — wpgsd-package","text":"Adjusted inference weighted parametric group sequential design. Weighted parametric group sequential design (WPGSD) Anderson et al. (2022) doi:10.1002/bimj.202100085  allows one take advantage known correlation structure constructing efficacy bounds control family-wise error rate (FWER) group sequential design. , correlation may due common observations nested populations, due common observations overlapping populations, due common observations control arm.","code":""},{"path":[]},{"path":"https://merck.github.io/wpgsd/reference/wpgsd-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"wpgsd: Weighted Parametric Group Sequential Design — wpgsd-package","text":"Maintainer: Yujie Zhao yujie.zhao@merck.com Authors: Keaven Anderson keaven_anderson@merck.com Zifang Guo zifang.guo@merck.com Jing Zhao jing_zhaox@merck.com Linda Sun linda_sun@merck.com Yi Cui yi.cui@merck.com Larry Leon larry.leon2@merck.com contributors: Merck Sharp & Dohme Corp [copyright holder]","code":""},{"path":"https://merck.github.io/wpgsd/news/index.html","id":"wpgsd-010","dir":"Changelog","previous_headings":"","what":"wpgsd 0.1.0","title":"wpgsd 0.1.0","text":"Initial release. wpgsd package now available GitHub, install prefer use specific version, install v0.1.0 GitHub release version number.","code":"remotes::install_github(\"Merck/wpgsd\") remotes::install_github(\"Merck/wpgsd@v0.1.0\")"}]
